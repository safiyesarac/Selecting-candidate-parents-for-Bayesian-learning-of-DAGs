{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Import <asia>\n",
      "[bnlearn] >Loading bif file </home/gulce/.pyenv/versions/3.11.7/lib/python3.11/site-packages/datazets/data/asia.bif>\n",
      "[bnlearn] >Check whether CPDs sum up to one.\n",
      "=== Data draw #1 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_0.txt\n",
      "=== Data draw #2 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_1.txt\n",
      "=== Data draw #3 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_2.txt\n",
      "=== Data draw #4 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_3.txt\n",
      "=== Data draw #5 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_4.txt\n",
      "=== Data draw #6 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_5.txt\n",
      "=== Data draw #7 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_6.txt\n",
      "=== Data draw #8 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_7.txt\n",
      "=== Data draw #9 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_8.txt\n",
      "=== Data draw #10 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_9.txt\n",
      "=== Data draw #11 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_10.txt\n",
      "=== Data draw #12 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_11.txt\n",
      "=== Data draw #13 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_12.txt\n",
      "=== Data draw #14 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_13.txt\n",
      "=== Data draw #15 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_14.txt\n",
      "=== Data draw #16 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_15.txt\n",
      "=== Data draw #17 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_16.txt\n",
      "=== Data draw #18 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_17.txt\n",
      "=== Data draw #19 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_18.txt\n",
      "=== Data draw #20 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_19.txt\n",
      "=== Data draw #21 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_20.txt\n",
      "=== Data draw #22 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_21.txt\n",
      "=== Data draw #23 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_22.txt\n",
      "=== Data draw #24 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_23.txt\n",
      "=== Data draw #25 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_24.txt\n",
      "=== Data draw #26 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_25.txt\n",
      "=== Data draw #27 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_26.txt\n",
      "=== Data draw #28 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_27.txt\n",
      "=== Data draw #29 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_28.txt\n",
      "=== Data draw #30 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_29.txt\n",
      "=== Data draw #31 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_30.txt\n",
      "=== Data draw #32 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_31.txt\n",
      "=== Data draw #33 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_32.txt\n",
      "=== Data draw #34 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_33.txt\n",
      "=== Data draw #35 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_34.txt\n",
      "=== Data draw #36 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_35.txt\n",
      "=== Data draw #37 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_36.txt\n",
      "=== Data draw #38 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_37.txt\n",
      "=== Data draw #39 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_38.txt\n",
      "=== Data draw #40 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_39.txt\n",
      "=== Data draw #41 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_40.txt\n",
      "=== Data draw #42 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_41.txt\n",
      "=== Data draw #43 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_42.txt\n",
      "=== Data draw #44 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_43.txt\n",
      "=== Data draw #45 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_44.txt\n",
      "=== Data draw #46 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_45.txt\n",
      "=== Data draw #47 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_46.txt\n",
      "=== Data draw #48 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_47.txt\n",
      "=== Data draw #49 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_48.txt\n",
      "=== Data draw #50 of 50 ===\n",
      "['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Command executed successfully! Output written to command_output.txt\n",
      "Command executed successfully! Output written to data/asia_sampled_variable_49.txt\n",
      "\n",
      "=== Result ===\n",
      "Estimated coverage across multiple data draws: 0.23226\n"
     ]
    }
   ],
   "source": [
    "import bnlearn as bn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "\n",
    "# Reduce the console logging level for these libraries\n",
    "logging.getLogger(\"bnlearn\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Data:\n",
    "    \"\"\"Simple container for the number of variables n.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "\n",
    "class GobnilpScores:\n",
    "    \"\"\"\n",
    "    A Scores-like class that wraps the output of parse_gobnilp_jkl \n",
    "    for use with sumu's candidate parent algorithms.\n",
    "    \"\"\"\n",
    "    def __init__(self, parsed_scores):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            parsed_scores (dict): \n",
    "                A dict of { node: [ (score, (parents...)), ... ], ... }\n",
    "        \"\"\"\n",
    "        self.n = max(parsed_scores.keys()) + 1\n",
    "        self.data = Data(self.n)\n",
    "        \n",
    "        # Store local scores in { node: {parents_tuple: score} }\n",
    "        self.local_scores = {}\n",
    "        for node, sp_list in parsed_scores.items():\n",
    "            self.local_scores[node] = {}\n",
    "            for (score, parents) in sp_list:\n",
    "                parents_sorted = tuple(sorted(parents))\n",
    "                self.local_scores[node][parents_sorted] = score\n",
    "\n",
    "        # If you do not have a known maximum parent set size, keep this -1\n",
    "        self.maxid = -1\n",
    "\n",
    "    def local(self, v, parents):\n",
    "        \"\"\"\n",
    "        Sumu calls 'scores.local(...)' in the candidate generation.\n",
    "        So, we must provide this method name exactly.\n",
    "        \"\"\"\n",
    "        p_sorted = tuple(sorted(parents))\n",
    "        return self.local_scores[v].get(p_sorted, float(\"-inf\"))\n",
    "\n",
    "    def all_candidate_restricted_scores(self, C):\n",
    "        \"\"\"\n",
    "        Return a dictionary: node -> {parents_tuple: local_score},\n",
    "        for parent sets that are subsets of C[node].\n",
    "        Handles both dict and np.ndarray for C.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        V = len(C)  # number of nodes\n",
    "        # Suppose each node i has M_i subsets...\n",
    "        # For it to be a uniform 2D array, you need the same number of columns for each node,\n",
    "        # often 2^|C[i]| if you consider all subsets, or something that sumu’s \"opt\" expects.\n",
    "        \n",
    "        # Let's say we do the maximum number of subsets among all i\n",
    "        max_subset_count = max(2 ** len(C[i]) for i in range(V))\n",
    "\n",
    "        # Make a big 2D array for (V, max_subset_count)\n",
    "        arr = np.full((V, max_subset_count), float(\"-inf\"), dtype=float)\n",
    "\n",
    "        # For each node i:\n",
    "        for i in range(V):\n",
    "            # Enumerate subsets of C[i] in some order\n",
    "            # Suppose subsets_i is a list of (subset_tuple, score)\n",
    "            subsets_i = []\n",
    "            for parents_tuple, sc in self.local_scores[i].items():\n",
    "                # only keep subsets that are within C[i]\n",
    "                if set(parents_tuple).issubset(C[i]):\n",
    "                    subsets_i.append((parents_tuple, sc))\n",
    "\n",
    "            # Sort them in a stable order and store them\n",
    "            # Typically sumu's \"opt\" uses bit-encodings, so you'd want j to match that encoding.\n",
    "            # For simplicity, let's just do enumerated:\n",
    "            for j, (parents_tuple, sc) in enumerate(subsets_i):\n",
    "                arr[i, j] = sc\n",
    "\n",
    "            # If subsets_i has fewer than max_subset_count, the rest remain -inf\n",
    "\n",
    "        return arr\n",
    "\n",
    "\n",
    "    def sum(self, v, U, T):\n",
    "\n",
    "        from itertools import combinations\n",
    "\n",
    "        # Compute the union of sets U and T\n",
    "        combined_parents = U | T  # This will create a single set containing all elements from U and T\n",
    "\n",
    "        # Determine the maximum number of parents (no limit in this case)\n",
    "        max_parents = len(combined_parents)\n",
    "\n",
    "        total_score = float(\"-inf\")\n",
    "\n",
    "        # Iterate over all possible parent sets from the union of U and T\n",
    "        for k in range(max_parents + 1):\n",
    "            for parent_set in combinations(combined_parents, k):\n",
    "                score = self.local(v, parent_set)\n",
    "                total_score = np.logaddexp(total_score, score)\n",
    "\n",
    "        return total_score\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"If your scoring logic uses caching, clear it here; otherwise do nothing.\"\"\"\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"If your scoring logic uses caching, clear it here; otherwise do nothing.\"\"\"\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "def parse_gobnilp_jkl(file_path):\n",
    "    \"\"\"\n",
    "    Parse the Gobnilp .jkl file format.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the .jkl file.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary where keys are the number of parents and values are tuples of (score, parent_nodes).\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    current_node = None\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # Skip empty lines\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) == 1 and parts[0].isdigit():\n",
    "                # Metadata line (e.g., \"8\"), skip it\n",
    "                continue\n",
    "            elif len(parts) == 2 and not line.startswith(\"-\"):\n",
    "                # Node header line (e.g., \"6 64\")\n",
    "                try:\n",
    "                    current_node = int(parts[0])  # Node ID\n",
    "                    scores[current_node] = []  # Initialize an empty list for this node\n",
    "                except ValueError:\n",
    "                    print(f\"Unexpected node header: {line}\")\n",
    "            elif current_node is not None and len(parts) >= 2:\n",
    "                # Score line, handle it\n",
    "                try:\n",
    "                    score = float(parts[0])  # First part is the score\n",
    "                    num_parents = int(parts[1])  # Second part is the number of parents\n",
    "                    parent_nodes = tuple(map(int, parts[2:])) if num_parents > 0 else ()\n",
    "                    scores[current_node].append((score, parent_nodes))\n",
    "                except ValueError:\n",
    "                    print(f\"Invalid score or parent set line: {line}\")\n",
    "            else:\n",
    "                print(f\"Unrecognized line: {line}\")\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "# 1. Coverage Fraction\n",
    "###########################\n",
    "def coverage_fraction(candidate_parents, sampled_dags):\n",
    "    \"\"\"\n",
    "    Returns the fraction of sampled DAGs whose parent sets\n",
    "    are all subsets of candidate_parents[node].\n",
    "    \"\"\"\n",
    "    count_covered = 0\n",
    "    total = len(sampled_dags)\n",
    "    \n",
    "    for dag in sampled_dags:\n",
    "        covered = True\n",
    "        for node, parents in dag.items():\n",
    "            # If the DAG's parents for this node go beyond what\n",
    "            # the candidate set allows, it's not covered\n",
    "            if not parents.issubset(candidate_parents[node]):\n",
    "                covered = False\n",
    "                break\n",
    "        if covered:\n",
    "            count_covered += 1\n",
    "    \n",
    "    return count_covered / total if total > 0 else 0.0\n",
    "from sumu.candidates import candidate_parent_algorithm as cpa\n",
    "def top_heuristic(scores,K):\n",
    "    \"\"\"\n",
    "    Example heuristic that returns \"every other node is a candidate parent.\"\n",
    "    Replace this with your real heuristic, e.g., sumu's 'pc', 'mb', etc.\n",
    "    \"\"\"\n",
    "    algo = cpa[\"top\"]\n",
    "    top_candidate_parents = algo(\n",
    "        K,\n",
    "        scores=scores,     # your sumu-compatible scoring object\n",
    "        n=scores.data.n    # number of nodes\n",
    "    )\n",
    "    return top_candidate_parents\n",
    "\n",
    "\n",
    "###########################\n",
    "# 2. Repeated Pipeline\n",
    "###########################\n",
    "def estimate_expected_coverage(\n",
    "    n_reps,            # how many datasets to sample\n",
    "    n_samples_data,    # how many data points in each dataset\n",
    "    # path to a .bif file or other BN structure\n",
    "      # function, e.g. coverage_fraction\n",
    "# function to learn/sample DAGs from data\n",
    "):\n",
    "    \"\"\"\n",
    "    Repeatedly:\n",
    "      1) Generate or sample data from a known BN model (or from some process).\n",
    "      2) Use 'heuristic_fn' to get candidate parents (optional).\n",
    "      3) Learn or sample DAGs (using 'dag_learning_fn').\n",
    "      4) Compute coverage fraction.\n",
    "    Return the average coverage fraction across multiple data draws.\n",
    "    \"\"\"\n",
    "    asia_model = bn.import_DAG('asia')\n",
    "\n",
    "\n",
    "    coverage_values = []\n",
    "    for rep in range(n_reps):\n",
    "        print(f\"=== Data draw #{rep+1} of {n_reps} ===\")\n",
    "\n",
    "        # 2) Simulate a fresh dataset from the BN\n",
    "        df_data = bn.sampling(asia_model, n=n_samples_data)\n",
    "        import pandas as pd\n",
    "        df= pd.DataFrame(df_data)\n",
    "        \n",
    "\n",
    "        columns = [str(i) for i in range(len(df.columns))]\n",
    "        print(columns)\n",
    "        data =[df[col].nunique() for col in df.columns]\n",
    "        data=[data]\n",
    "        # data = [[2] *( len(columns)+0)]\n",
    "        # Convert to DataFrame\n",
    "        df_arity = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "        column_mapping = {old: new for old, new in zip(df.columns, df_arity.columns)}\n",
    "        df = df.rename(columns=column_mapping)\n",
    "\n",
    "        # Now, combine the data as before\n",
    "        df_combined_correct = pd.concat([df_arity, df], ignore_index=True)\n",
    "        df_combined_correct.to_csv(\"data/asia_dataset_variable_\"+str(rep)+\".csv\", index=False)\n",
    "\n",
    "\n",
    "        # Combine the arity information (first row) with the rest of the dataset\n",
    "        df_combined_correct = pd.concat([df_arity, df])\n",
    "        # Save the DataFrame to a CSV file for inspection or future use\n",
    "        df_arity.to_csv(\"data/asia_dataset_variable_\"+str(rep)+\".csv\", index=False)\n",
    "        # Append the arity row (df_arity) on top of the data_samples\n",
    "        df_combined = pd.concat([df_arity, df], ignore_index=True)\n",
    "\n",
    "        # Save the combined dataset to a CSV file\n",
    "        df_combined.to_csv(\"data/asia_dataset_variable_\"+str(rep)+\".dat\", index=False, sep=' ')\n",
    "\n",
    "\n",
    "        \n",
    "        import subprocess\n",
    "\n",
    "        # Define the command as a list\n",
    "        command = [\n",
    "            \"python3\",\n",
    "            \"./pygobnilp-1.0/rungobnilp.py\",\n",
    "            \"./data/asia_dataset_variable_\"+str(rep)+\".dat\",\n",
    "            \"--output_scores\", \"data/asia_scores_variable_\"+str(rep)+\".jkl\",\n",
    "            \"--score\", \"BDeu\",\n",
    "            \"--nopruning\",\n",
    "            \"--end\", \"local scores\"\n",
    "        ]\n",
    "\n",
    "        # Specify the output file\n",
    "        output_file = \"command_output.txt\"\n",
    "\n",
    "        # Run the command and write its output to a file\n",
    "        with open(output_file, \"w\") as file:\n",
    "            try:\n",
    "                result = subprocess.run(command, check=True, text=True, stdout=file, stderr=subprocess.PIPE)\n",
    "                print(\"Command executed successfully! Output written to\", output_file)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(\"An error occurred while executing the command.\")\n",
    "                print(\"Error message:\", e.stderr)\n",
    "\n",
    "        # 3) If your candidate parent heuristic depends on data, compute it here:\n",
    "       \n",
    "\n",
    "        # 1. Parse Gobnilp output\n",
    "        parsed_scores = parse_gobnilp_jkl(\"data/asia_scores_variable_\"+str(rep)+\".jkl\")\n",
    "        scores = GobnilpScores(parsed_scores)\n",
    "\n",
    "        candidate_parents = top_heuristic(scores,5)\n",
    "\n",
    "        sampled_dags = generate_sampled_dags(rep)\n",
    "  \n",
    "        # 5) Compute coverage fraction for this data’s DAGs\n",
    "        fraction = coverage_fraction(candidate_parents, sampled_dags)\n",
    "        coverage_values.append(fraction)\n",
    "\n",
    "    # 6) Average coverage fraction across all data draws\n",
    "    return np.mean(coverage_values)\n",
    "\n",
    "###########################\n",
    "# 3. Demo Heuristic / DAG Learning\n",
    "###########################\n",
    "\n",
    "# Suppose your “Back-forth candidates” mean that for node i,\n",
    "# the candidate parents are exactly this set.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def coverage_fraction(candidate_parents, sampled_dags):\n",
    "    \"\"\"\n",
    "    Returns the fraction of sampled DAGs whose parent sets\n",
    "    are all subsets of candidate_parents[node].\n",
    "    \"\"\"\n",
    "    count_covered = 0\n",
    "    total = len(sampled_dags)\n",
    "    \n",
    "    for dag in sampled_dags:\n",
    "        covered = True\n",
    "        for node, parents in dag.items():\n",
    "            # If the DAG's parents for this node go beyond what\n",
    "            # the candidate set allows, it's not covered\n",
    "            if not parents.issubset(candidate_parents[node]):\n",
    "                covered = False\n",
    "                break\n",
    "        if covered:\n",
    "            count_covered += 1\n",
    "    \n",
    "    return count_covered / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def generate_sampled_dags(rep):\n",
    "    \"\"\"\n",
    "    Example DAG-learning function that returns:\n",
    "       1) an empty DAG, plus\n",
    "       2) a random DAG with one parent assigned arbitrarily.\n",
    "\n",
    "    Replace with Gobnilp, R MCMC, GES, or any real pipeline.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "\n",
    "    # Define the command as a list of arguments\n",
    "    command = [\"./modular-dag-sampling-master/sampler\", \"nonsymmetric\", \"data/asia_scores_variable_\"+str(rep)+\".jkl\", \"10000\"]\n",
    "\n",
    "    # Specify the output file\n",
    "    output_file = \"data/asia_sampled_variable_\"+str(rep)+\".txt\"\n",
    "\n",
    "    # Run the command and write its output to the file\n",
    "    with open(output_file, \"w\") as file:\n",
    "        try:\n",
    "            result = subprocess.run(command, check=True, text=True, stdout=file, stderr=subprocess.PIPE)\n",
    "            print(\"Command executed successfully! Output written to\", output_file)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(\"An error occurred while executing the command.\")\n",
    "            print(\"Error message:\", e.stderr)\n",
    "            \n",
    "            \n",
    "    def parse_dag_line(line: str) -> dict:\n",
    "    \n",
    "        \n",
    "        dag = {}\n",
    "        \n",
    "        # 1) Split by \"},\" to separate each node's parent specification\n",
    "        chunks = line.split(\"},\")\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk = chunk.strip()\n",
    "            if not chunk:\n",
    "                continue  # skip empty pieces (can happen if line ends with \"},\")\n",
    "            \n",
    "            # 2) Ensure the chunk ends with \"}\" if it doesn't already\n",
    "            if not chunk.endswith(\"}\"):\n",
    "                chunk += \"}\"\n",
    "            \n",
    "            # Now chunk should look like, for example: \"0 <- {}\", or \"1 <- {3}\"\n",
    "            if \"<-\" not in chunk:\n",
    "                # If there's no \"<-\", it's not valid for our parser;\n",
    "                # skip or raise an error, as you prefer.\n",
    "                continue\n",
    "            \n",
    "            node_str, parents_str = chunk.split(\"<-\")\n",
    "            node_str = node_str.strip()        # e.g. \"0\"\n",
    "            parents_str = parents_str.strip()  # e.g. \"{3}\" or \"{}\"\n",
    "            \n",
    "            # 3) Remove the outer braces from parents_str\n",
    "            if parents_str.startswith(\"{\"):\n",
    "                parents_str = parents_str[1:]\n",
    "            if parents_str.endswith(\"}\"):\n",
    "                parents_str = parents_str[:-1]\n",
    "            \n",
    "            # 4) Parse the parent nodes inside the braces\n",
    "            parents_str = parents_str.strip()\n",
    "            if parents_str:\n",
    "                # e.g. \"1, 3\" -> [\"1\", \"3\"] -> {1, 3}\n",
    "                parent_list = [p.strip() for p in parents_str.split(\",\") if p.strip()]\n",
    "                parents = set(int(p) for p in parent_list)\n",
    "            else:\n",
    "                parents = set()\n",
    "            \n",
    "            # Convert node_str to int (assuming your node labels are numeric)\n",
    "            node = int(node_str)\n",
    "            \n",
    "            # Store in the DAG dict\n",
    "            dag[node] = parents\n",
    "        \n",
    "        return dag\n",
    "\n",
    "    def parse_dag_file(file_path: str) -> list:\n",
    "        \"\"\"\n",
    "        Read the file line by line. Each line describes one DAG,\n",
    "        and parse it into a dict: { node: set_of_parents, ... }.\n",
    "        \n",
    "        Returns a list of such dicts, e.g. [dag1, dag2, dag3, ...].\n",
    "        \"\"\"\n",
    "        all_dags = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line_no, line in enumerate(f, start=1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue  # skip empty lines\n",
    "\n",
    "                dag = parse_dag_line(line)\n",
    "                if dag:\n",
    "                    # If your line is valid, append it\n",
    "                    all_dags.append(dag)\n",
    "                else:\n",
    "                    # You can decide how to handle an empty / invalid parse\n",
    "                    print(f\"Warning: line {line_no} didn't parse to a DAG, skipping.\")\n",
    "        \n",
    "        return all_dags\n",
    "\n",
    "\n",
    "\n",
    "    filename = \"data/asia_sampled_variable_\"+str(rep)+\".txt\"\n",
    "    sampled_dags = parse_dag_file(filename)\n",
    "    return sampled_dags\n",
    "    \n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "###########################\n",
    "# 4. Run the Test\n",
    "###########################\n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "\n",
    "    # We do 3 repeated draws, each time sampling 500 data points\n",
    "    n_reps = 50\n",
    "    n_samples_data = 500\n",
    "\n",
    "    # Call the repeated pipeline\n",
    "    avg_coverage = estimate_expected_coverage(\n",
    "        n_reps=n_reps,\n",
    "        n_samples_data=n_samples_data,\n",
    "\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Result ===\")\n",
    "    print(\"Estimated coverage across multiple data draws:\", avg_coverage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
