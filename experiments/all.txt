# mybnexp/coverage.py

def coverage_fraction(candidate_parents, sampled_dags):
    """
    Compute fraction of sampled_dags that are covered by candidate_parents.

    Coverage criterion:
        For every node v, the DAG's parents for v are a subset of candidate_parents[v].
    """
    if not sampled_dags:
        return 0.0
    count_covered = 0
    total = len(sampled_dags)

    for dag in sampled_dags:
        covered = True
        for node, parents in dag.items():
            if not parents.issubset(candidate_parents[node]):
                covered = False
                break
        if covered:
            count_covered += 1
    
    return count_covered / total

# mybnexp/data_io.py

import logging
import pandas as pd
import sumu


def read_csv_as_sumu_data(csv_path, skiprows=None, discrete=True):
    """
    Reads a CSV file into a sumu.Data object.
    
    Args:
        csv_path (str): Path to CSV file.
        skiprows (list[int] or None): Rows to skip in the CSV (e.g. [1] if row2 is metadata).
        discrete (bool): Whether the data is discrete for sumu.
    
    Returns:
        sumu.Data
    """
    df = pd.read_csv(csv_path, skiprows=skiprows)
    data_matrix = df.values
    logging.info(f"Loaded CSV data from {csv_path} with shape {data_matrix.shape}")
    return sumu.Data(data_matrix, discrete=discrete)


def parse_dag_line(line: str) -> dict:
    """
    Given a line like:
        '0 <- {}, 1 <- {3}, 2 <- {3}, 3 <- {}, ...'
    return a dict {0: set(), 1: {3}, 2: {3}, 3: set(), ...}.
    """
    dag = {}
    chunks = line.split("},")
    for chunk in chunks:
        chunk = chunk.strip()
        if not chunk:
            continue
        if not chunk.endswith("}"):
            chunk += "}"
        if "<-" not in chunk:
            continue

        node_str, parents_str = chunk.split("<-")
        node_str = node_str.strip()
        parents_str = parents_str.strip()

        # Remove outer braces
        if parents_str.startswith("{"):
            parents_str = parents_str[1:]
        if parents_str.endswith("}"):
            parents_str = parents_str[:-1]

        if parents_str.strip():
            parent_list = [p.strip() for p in parents_str.split(",") if p.strip()]
            parents = set(int(p) for p in parent_list)
        else:
            parents = set()

        dag[int(node_str)] = parents
    return dag


def parse_dag_file(dag_file: str) -> list:
    """
    Read each line from dag_file, parse into a DAG dict: {node: set_of_parents}.
    Returns a list of such DAGs.
    """
    all_dags = []
    with open(dag_file, "r") as f:
        for line_no, line in enumerate(f, start=1):
            line = line.strip()
            if not line:
                continue
            dag = parse_dag_line(line)
            if dag:
                all_dags.append(dag)
            else:
                logging.warning(f"Line {line_no} in {dag_file} was empty or invalid.")
    logging.info(f"Parsed {len(all_dags)} DAGs from {dag_file}")
    return all_dags


def parse_gobnilp_jkl(file_path: str) -> dict:
    """
    Parse a Gobnilp .jkl file.
    Returns a dict of node -> [ (score, (parents...)), (score, (parents...)), ... ].
    """
    scores = {}
    current_node = None

    with open(file_path, 'r') as file:
        for line in file:
            line = line.strip()
            if not line:
                continue
            parts = line.split()
            if len(parts) == 1 and parts[0].isdigit():
                # Possibly a metadata line, skip
                continue
            elif len(parts) == 2 and not line.startswith("-"):
                # Node header line (e.g., "6 64")
                try:
                    current_node = int(parts[0])
                    scores[current_node] = []
                except ValueError:
                    logging.warning(f"Unexpected node header: {line}")
            elif current_node is not None and len(parts) >= 2:
                try:
                    score = float(parts[0])
                    num_parents = int(parts[1])
                    parent_nodes = tuple(map(int, parts[2:])) if num_parents > 0 else ()
                    scores[current_node].append((score, parent_nodes))
                except ValueError:
                    logging.warning(f"Invalid line for score/parents: {line}")
            else:
                logging.warning(f"Unrecognized line: {line}")

    logging.info(f"Parsed Gobnilp .jkl file: {file_path}, found {len(scores)} nodes.")
    return scores
import bnlearn as bn
   
import pandas as pd
from pgmpy.readwrite import BIFReader

def sample_datapoints_from_model(model_file, n,csv_file, dat_file):


    # Load a predefined Bayesian network structure
    model = bn.import_DAG(model_file)

    model_data = bn.sampling(model, n=n)
    df=pd.DataFrame(model_data)
    columns = [str(i) for i in range(len(df.columns))]
    print(columns)
    data =[df[col].nunique() for col in df.columns]
    data=[data]
    # data = [[2] *( len(columns)+0)]
    # Convert to DataFrame
    df_arity = pd.DataFrame(data, columns=columns)

    column_mapping = {old: new for old, new in zip(df.columns, df_arity.columns)}
    df = df.rename(columns=column_mapping)

    # Now, combine the data as before
    df_combined_correct = pd.concat([df_arity, df], ignore_index=True)
    df_combined_correct.to_csv(csv_file, index=False)


    # Combine the arity information (first row) with the rest of the dataset
    df_combined_correct = pd.concat([df_arity, df])
    # Save the DataFrame to a CSV file for inspection or future use
    #df_arity.to_csv("data/original_hailfinder_dataset.csv", index=False)
    # Append the arity row (df_arity) on top of the data_samples
    df_combined = pd.concat([df_arity, df], ignore_index=True)

    # Save the combined dataset to a CSV file
    df_combined.to_csv(dat_file, index=False, sep=' ')


def save_data(df,csv_file, dat_file):
    columns = [str(i) for i in range(len(df.columns))]
    print(columns)
    data =[df[col].nunique() for col in df.columns]
    data=[data]
    # data = [[2] *( len(columns)+0)]
    # Convert to DataFrame
    df_arity = pd.DataFrame(data, columns=columns)

    column_mapping = {old: new for old, new in zip(df.columns, df_arity.columns)}
    df = df.rename(columns=column_mapping)

    # Now, combine the data as before
    df_combined_correct = pd.concat([df_arity, df], ignore_index=True)
    df_combined_correct.to_csv(csv_file, index=False)


    # Combine the arity information (first row) with the rest of the dataset
    df_combined_correct = pd.concat([df_arity, df])
    # Save the DataFrame to a CSV file for inspection or future use
    #df_arity.to_csv("data/original_hailfinder_dataset.csv", index=False)
    # Append the arity row (df_arity) on top of the data_samples
    df_combined = pd.concat([df_arity, df], ignore_index=True)

    # Save the combined dataset to a CSV file
    df_combined.to_csv(dat_file, index=False, sep=' ')    
    
def compute_bdeu_scores(dat_file,jkl_file):
    import subprocess

    # Define the command as a list
    command = [
        "python3",
        "/home/gulce/Downloads/thesis/pygobnilp-1.0/rungobnilp.py",
       dat_file,
        "--output_scores", jkl_file,
        "--score", "BDeu",
        "--nopruning",# "--palim","1",
        "--end", "local scores"
    ]

    # Specify the output file
    output_file = "command_output.txt"

    # Run the command and write its output to a file
    with open(output_file, "w") as file:
        try:
            result = subprocess.run(command, check=True, text=True, stdout=file, stderr=subprocess.PIPE)
            print("Command executed successfully! Output written to", output_file)
        except subprocess.CalledProcessError as e:
            print("An error occurred while executing the command.")
            print("Error message:", e.stderr)

# sample_datapoints_from_model('/home/gulce/Downloads/thesis/data/child/child.bif',500,  '/home/gulce/Downloads/thesis/data/child/child_1000.csv','/home/gulce/Downloads/thesis/data/child/child_500.dat')
# compute_bdeu_scores( '/home/gulce/Downloads/thesis/data/child/child_500.dat', '/home/gulce/Downloads/thesis/data/child/child_500.jkl')


# sample_datapoints_from_model('/home/gulce/Downloads/thesis/data/synt/synt.bif',10000,  '/home/gulce/Downloads/thesis/data/synt/synt.csv','/home/gulce/Downloads/thesis/data/synt/synt.dat')
# compute_bdeu_scores( '/home/gulce/Downloads/thesis/data/synt/synt.dat', '/home/gulce/Downloads/thesis/data/synt/synt.jkl')# takes jkl file k not gıven all k gıven then a txt fıle to measure coverage fractıon saves logs in a log fıle the results 
#!/usr/bin/env python3

"""
experiment_heuristics_with_timeout.py

This script:
  1) Reads a Gobnilp .jkl file (local scores) and parses it into a sumu-compatible Scores object.
  2) Reads a text file of sampled DAGs to compute coverage fraction against.
  3) Optionally reads a dataset CSV if needed by certain sumu algorithms (like 'mb', 'pc', 'ges').
  4) Runs several candidate-parent heuristics from sumu (and a custom beam search example) 
     for K in [K_min..K_max].
  5) Computes coverage fraction and logs results to a CSV file and prints to stdout.
  6) Skips any heuristic call that exceeds --skip_after_seconds (default 300s = 5 min).

Usage:
  python experiment_heuristics_with_timeout.py \
      --jkl_file data/hailfinder_scores.jkl \
      --sampled_dags data/hailfinder_sampled_dags.txt \
      --data_file data/hailfinder_dataset.csv \
      --K_min 1 \
      --K_max 30 \
      --skip_after_seconds 300 \
      --output_csv coverage_log.csv


"""

import argparse
import time
import numpy as np
import pandas as pd
import os
import data_io
import heuristics
print(heuristics.__file__)
import coverage 

# sumu and related modules
import sumu
from sumu.candidates import candidate_parent_algorithm as cpa


#(base) gulce@gulce-HP-Laptop:~/Downloads/thesis$ python experiments/heuristic_performance_experiments.py     --jkl_file data/asia_scores.jkl     --sampled_dags data/asia_sampled.txt     --data_file data/asia_dataset.csv     --K_min 1     --K_max 7    --skip_after_seconds 300     --output_csv data/coverage/asia_coverage_results.csv
#
##############################################################################
# 6. main()
##############################################################################
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--jkl_file", type=str, required=True,
                        help="Path to the Gobnilp .jkl scores file.")
    parser.add_argument("--sampled_dags", type=str, required=True,
                        help="Path to the text file containing sampled DAGs.")
    parser.add_argument("--data_file", type=str, default=None,
                        help="Path to a CSV data file (optional), needed for certain sumu heuristics (mb, pc, ges, etc.).")
    parser.add_argument("--K_min", type=int, default=1,
                        help="Minimum K for candidate parents.")
    parser.add_argument("--K_max", type=int, default=10,
                        help="Maximum K for candidate parents.")
    parser.add_argument("--output_csv", type=str, default="coverage_log.csv",
                        help="Path to output CSV with coverage fraction results.")
    parser.add_argument("--sample_mode", type=str, default="exact modular",
                        help="exact sampling or mcmc")
    parser.add_argument("--skip_after_seconds", type=int, default=300,
                        help="Time limit (in seconds) for each heuristic call. If exceeded, skip.")
    args = parser.parse_args()

    # 1. Parse JKL -> GobnilpScores
    parsed_scores = data_io.parse_gobnilp_jkl(args.jkl_file)
    scores = heuristics.GobnilpScores(parsed_scores)
    n = scores.n

    # 2. Parse sampled DAGs
    sampled_dags = data_io.parse_dag_file(args.sampled_dags)

    # 3. Optionally load data for sumu algorithms that require the original data
    #    We'll also store the number of data rows in `num_data_rows`.
    mydata = None
    num_data_rows = 0
    if args.data_file and os.path.exists(args.data_file):
        # If the second row has e.g. arity or an extraneous row, adapt skiprows if needed:
        # df = pd.read_csv(args.data_file, skiprows=[1])
   
        df = pd.read_csv(args.data_file, skiprows=[1])

        mydata = sumu.Data(df.values)  
        num_data_rows = df.shape[0]
        print(f"[INFO] Loaded data file {args.data_file} with {num_data_rows} rows.")
    else:
        print("No data file given or file does not exist; 'mb', 'pc', 'ges' etc. might fail if used.")
    print(cpa.keys())
    # 4. Define the candidate algorithms we want to test
    candidate_algos = {
        "top":         (cpa["top"],         {"scores": scores, "n": n}),
         "opt":         (cpa["opt"],         {"scores": scores, "n": n}),
        # "mb":          (cpa["mb"],          {"data": mydata, "fill": "random"}),
        # "pc":          (cpa["pc"],          {"data": mydata, "fill": "random"}),
        # "ges":         (cpa["ges"],         {"scores": scores, "data": mydata, "fill": "top"}),
        "greedy":      (cpa["greedy"],      {"scores": scores}),
        # "greedy-lite": (cpa["greedy-lite"], {"scores": scores}),
        # "back-forth":  (cpa["back-forth"],  {"scores": scores, "data": scores.data}),
        # "beam":        (heuristics.beam_bdeu,          {"scores": scores, "beam_size": 5}),
        # "marginal_bdeu_parents":        (heuristics.marginal_bdeu_parents,            {"scores": scores, "n": n}),
        
        #  "voting_bdeu_parents":        (heuristics.bdeu_score_based_voting,            {"scores": scores}),
        #  "synergy": (heuristics.synergy_based_parent_selection,  {"scores": scores}),
        # "stability":(heuristics.stability_bdeu, {"scores": scores, "data": mydata}),
        "post":         (heuristics.maximize_true_graph_posterior,         {"scores": scores}),
        
        
        
        
        
    }

    # 5. Loop over each algorithm, vary K, measure coverage, respect time limit
    #    We'll now store the number of data rows in the results, too.
    results = []  # will store tuples of (algorithm, K, coverage_fraction, num_data_rows)

    for algo_name, (algo_func, algo_kwargs) in candidate_algos.items():
        print(f"\n*** Running algorithm: {algo_name} ***")
        for K in range(args.K_min, args.K_max + 1):
            print(f"   [K={K}] ...", end="", flush=True)
            start_time = time.time()
            candidate_parents = None

            # Attempt to run the heuristic
            try:
                tmp_result = algo_func(K, **algo_kwargs)
                # Some sumu CPAs return (C, None) or (C, extra).
                if isinstance(tmp_result, tuple) and len(tmp_result) >= 1:
                    candidate_parents = tmp_result[0]
                else:
                    candidate_parents = tmp_result
            except Exception as e:
                # Could be an error if data wasn't provided for 'mb' or 'pc', or other issues
                print(f"  [ERROR] {e}")
                results.append((algo_name, K, None, num_data_rows))
                continue

            # Check elapsed time
            elapsed = time.time() - start_time
            if elapsed > args.skip_after_seconds:
                # If it took more than skip_after_seconds, skip it
                print(f"  [SKIPPED: took {elapsed:.1f}s > {args.skip_after_seconds}s]")
                results.append((algo_name, K, None, num_data_rows))
                break

            # Otherwise, measure coverage
            cf = coverage.coverage_fraction(candidate_parents, sampled_dags)
            print(f"  coverage={cf}, time={elapsed:.1f}s")
            results.append((algo_name, K, cf, num_data_rows))

    # 6. Save results to CSV
    df_res = pd.DataFrame(results, columns=["Algorithm", "K", "CoverageFraction", "NumDataRows"])
    df_res.to_csv(args.output_csv, index=False)
    print(f"\nCoverage results saved to: {args.output_csv}")
    if num_data_rows > 0:
        print(f"All coverage fractions above used dataset with {num_data_rows} rows.")
    else:
        print("No dataset was loaded (NumDataRows=0).")


if __name__ == "__main__":
    main()



"""
experiment_consensus_data.py

Workflow:
  1) Read a file of sampled DAGs (each DAG is an adjacency matrix in text).
  2) Form a consensus DAG by thresholding edge frequencies.
  3) Assign discrete CPDs for that consensus DAG, then sample synthetic data.
  4) Compute BDeu local scores from the synthetic dataset for each node + possible parent set.
  5) Write these local scores to a Gobnilp-style .jkl file.
  6) Run your existing coverage fraction experiment script by passing
     the generated .jkl file and the DAG file you want to measure coverage against.
"""

import argparse
import numpy as np
import pandas as pd
import random
import os
import logging

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# --------------------------------------------------------------------------
# 1) Exact Coverage
# --------------------------------------------------------------------------
def exact_coverage(true_parents, heuristic_output):
    """
    Returns the fraction of nodes for which the heuristic
    exactly matches the true parent set for that node.
    """
    coverage = []
    for node, true_parent_set in true_parents.items():
        # Convert to tuple for consistent comparison
        true_parent_set = tuple(true_parent_set) if isinstance(true_parent_set, list) else true_parent_set

        # 'heuristic_output.get(node, [])' should be a list/tuple of candidate sets
        found = any(np.array_equal(true_parent_set, heuristic_set)
                    for heuristic_set in heuristic_output.get(node, []))
        coverage.append(1 if found else 0)
    return np.mean(coverage)

# --------------------------------------------------------------------------
# 2) Average Parent Coverage (node-by-node coverage of true parents)
# --------------------------------------------------------------------------
def average_parent_coverage(true_parents, heuristic_output):
    """
    For each node:
      - let T = true parent set
      - let P = predicted parent set
      Coverage for that node = |T ∩ P| / |T| (the fraction of true parents included)
      If T is empty, coverage is 1.0 for that node.

    Returns the average coverage across all nodes.
    """
    total_coverage = 0.0
    n = len(true_parents)

    for node, true_set in true_parents.items():
        pred_set = heuristic_output.get(node, ())
        if len(true_set) == 0:
            # If no true parents, count coverage as 1
            total_coverage += 1.0
        else:
            intersection_size = len(set(true_set).intersection(pred_set))
            total_coverage += intersection_size / len(true_set)

    return total_coverage / n

# --------------------------------------------------------------------------
# 3) Precision, Recall, F1
# --------------------------------------------------------------------------
def precision_recall_f1(true_parents, heuristic_output):
    """
    Edge-based precision/recall: each (parent->child) is an edge.
    """
    true_edges = set()
    heuristic_edges = set()

    # Collect edges from 'true_parents'
    for node, parents in true_parents.items():
        for parent in parents:
            true_edges.add((parent, node))

    # Collect edges from 'heuristic_output'
    for node, parents in heuristic_output.items():
        for parent in parents:
            heuristic_edges.add((parent, node))

    tp = len(true_edges & heuristic_edges)
    fp = len(heuristic_edges - true_edges)
    fn = len(true_edges - heuristic_edges)

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    return precision, recall, f1

# --------------------------------------------------------------------------
# 4) Structural Hamming Distance (SHD)
# --------------------------------------------------------------------------
def structural_hamming_distance(true_parents, heuristic_output):
    """
    SHD = # of edges that differ between the true graph and the predicted graph.
    (Counting each mismatch in direction or presence/absence.)
    """
    true_edges = set()
    heuristic_edges = set()

    for node, parents in true_parents.items():
        for parent in parents:
            true_edges.add((parent, node))

    for node, parents in heuristic_output.items():
        for parent in parents:
            heuristic_edges.add((parent, node))

    return len(true_edges.symmetric_difference(heuristic_edges))

# --------------------------------------------------------------------------
# 5) Rank-Based Metric
# --------------------------------------------------------------------------
def rank_coverage(true_parents, heuristic_output):
    """
    If 'heuristic_output[node]' is a list of candidate sets in some rank order,
    find the position of the true parent set. Then average across nodes.
    """
    ranks = []
    for node, true_parent_set in true_parents.items():
        heuristic_parent_sets = heuristic_output.get(node, [])
        try:
            rank = heuristic_parent_sets.index(true_parent_set) + 1
        except ValueError:
            rank = len(heuristic_parent_sets) + 1
        ranks.append(rank)
    return np.mean(ranks)

# --------------------------------------------------------------------------
# 6) Adjacency -> Dictionary
# --------------------------------------------------------------------------
def adj_matrix_to_dict(adj_matrix):
    """
    Convert adjacency matrix into {node: (list_of_parents...)}.
    """
    n = adj_matrix.shape[0]
    consensus_dict = {}
    for node in range(n):
        parent_indices = np.where(adj_matrix[:, node] == 1)[0]
        consensus_dict[node] = tuple(parent_indices)
    return consensus_dict

# --------------------------------------------------------------------------
# 7) Sample Data
# --------------------------------------------------------------------------
def topological_sort(adj):
    n = adj.shape[0]
    in_degree = np.sum(adj, axis=0)
    queue = [i for i in range(n) if in_degree[i] == 0]
    order = []
    while queue:
        node = queue.pop()
        order.append(node)
        for j in range(n):
            if adj[node, j] == 1:
                in_degree[j] -= 1
                if in_degree[j] == 0:
                    queue.append(j)
    if len(order) < n:
        raise ValueError("Not a DAG (cycle found).")
    return order
from typing import Dict, Set, List

def sample_data_from_dag_discrete(adj, n_samples=1000, arity=2, seed=42):
    import random
    random.seed(seed)
    np.random.seed(seed)

    n = adj.shape[0]
    topo = topological_sort(adj)

    # Build CPDs
    cpd = {}
    parent_index_func = {}

    for node in topo:
        parents = np.where(adj[:, node] == 1)[0]
        k = len(parents)
        n_parent_configs = arity**k
        table = np.random.rand(n_parent_configs, arity)
        table /= table.sum(axis=1, keepdims=True)
        cpd[node] = table

        def make_index_function(parents_list):
            def index_of(parent_vals):
                idx = 0
                for val in parent_vals:
                    idx = idx * arity + val
                return idx
            return index_of
        parent_index_func[node] = make_index_function(parents)

    data = np.zeros((n_samples, n), dtype=int)
    for node in topo:
        parents = np.where(adj[:, node] == 1)[0]
        idx_func = parent_index_func[node]
        for s in range(n_samples):
            pvals = data[s, parents]
            row_idx = idx_func(tuple(pvals))
            probs = cpd[node][row_idx, :]
            x_val = np.random.choice(np.arange(arity), p=probs)
            data[s, node] = x_val
    return data, cpd

########################
# (B) Parameter Sampling
#########################

def sample_cpts_for_dag(
    dag: dict, num_states: int, alpha: float = 1.0
) -> dict:
    """
    Given a DAG (dict: node -> set_of_parents), sample each node's CPT from a Dirichlet(alpha) prior.
    """
    cpts = {}
    d = len(dag)
    for node in range(d):
        parents = dag[node]
        num_parent_combos = num_states ** len(parents)
        cpt = np.zeros((num_parent_combos, num_states))
        for row_idx in range(num_parent_combos):
            theta = np.random.gamma(alpha, 1.0, size=num_states)
            theta /= theta.sum()  # normalize
            cpt[row_idx, :] = theta
        cpts[node] = cpt
    return cpts


#########################
# (C) Data Generation   #
#########################

def get_topological_order(dag: Dict[int, Set[int]]) -> List[int]:
    """
    Returns a topological ordering of the DAG (node->parents).
    Simple BFS/Kahn's algorithm or DFS-based. 
    """
    d = len(dag)
    in_degree = {node: 0 for node in range(d)}
    for child, parents in dag.items():
        for p in parents:
            in_degree[child] += 1
    
    queue = [n for n in range(d) if in_degree[n] == 0]
    topo_order = []
    while queue:
        cur = queue.pop()
        topo_order.append(cur)
        # "Remove" cur
        for child, parents in dag.items():
            if cur in parents:
                in_degree[child] -= 1
                if in_degree[child] == 0:
                    queue.append(child)
    
    if len(topo_order) != d:
        raise ValueError("Graph is not acyclic or an error occurred. Cannot find topological order.")
    return topo_order

def generate_data_from_dag(
    dag: dict,
    cpts: dict,
    num_samples: int,
    num_states: int
) -> np.ndarray:
    """
    Generate synthetic data from a DAG with known CPTs.
    """
    d = len(dag)
    data = np.zeros((num_samples, d), dtype=int)
    topo_order = get_topological_order(dag)
    for s in range(num_samples):
        row_vals = [None] * d
        for node in topo_order:
            parents = dag[node]
            parent_list = sorted(parents)
            parent_idx = 0
            for p in parent_list:
                parent_idx = parent_idx * num_states + row_vals[p]
            probs = cpts[node][parent_idx, :]
            child_val = np.random.choice(num_states, p=probs)
            row_vals[node] = child_val
        data[s, :] = row_vals
    return data

# --------------------------------------------------------------------------
# 8) Main
# --------------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--n_samples_data", type=int, default=10,
                        help="Number of data samples to generate.")
    parser.add_argument("--arity", type=int, default=2,
                        help="Discrete arity for each variable.")
    parser.add_argument("--log_csv", type=str, default="coverage_log.csv",
                        help="Where to store coverage fraction results.")
    # Add arguments for example
    parser.add_argument("--alpha", type=float, default=1.0,
                        help="Dirichlet prior parameter.")
    parser.add_argument("--data_in", type=str, 
                        default="/home/gulce/Downloads/thesis/data/fair/test.txt",
                        help="File containing sampled DAGs.")
    parser.add_argument("--replicates", type=int, default=1,
                        help="Number of replicate data sets per DAG.")
    parser.add_argument("--sample_sizes", type=str, default="1000,5000",
                        help="Comma-separated list of sample sizes to test.")
    args = parser.parse_args()
    import sampling
    sampling.sample_from_exact_modular_fair_sampler(25,15,"/home/gulce/Downloads/thesis/data/fair/test.txt")
    
    # Convert string to list of int
    sample_sizes = [int(x) for x in args.sample_sizes.split(",")]
    replicates = args.replicates
    alpha = args.alpha
    num_states = args.arity

    # ------------------------------------------------------------------
    # [1] Read or sample DAGs
    # Here you have your own logic to read 'sampled_dags' from a file.
    # Suppose data_io.parse_dag_file(...) returns a list of adjacency dicts:
    import data_io
    # e.g. each element is {node: [parents...], ...}
    sampled_dags = data_io.parse_dag_file(args.data_in)
    logging.info(f"Loaded {len(sampled_dags)} DAGs from {args.data_in}")

    # We will store *all results* for all DAGs into this list,
    # then average across DAGs at the very end.
    all_results = []

    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ------------------------------------------------------------------
    # For each DAG in your file
    for dag_index, true_dag in enumerate(sampled_dags):
        d = len(true_dag)

        # For each requested sample size and replicate
        for size in sample_sizes:
            
                logging.info(f"[DAG #{dag_index}] sample_size={size}, rep=1")

                # [2] Sample parameters for the DAG
                cpts = sample_cpts_for_dag(true_dag, num_states=num_states, alpha=alpha)

                # [3] Generate data
                data_mat = generate_data_from_dag(
                    dag=true_dag,
                    cpts=cpts,
                    num_samples=size,
                    num_states=num_states
                )

                # [4] Compute local BDeu scores -> produce .jkl
                # Save data into .csv/.dat
                import pandas as pd
                df_data = pd.DataFrame(data_mat)
                
                # Build unique filenames for each DAG/rep
                out_csv = f"/home/gulce/Downloads/thesis/data/fair/tmp/consensus_dag_{dag_index}_{timestamp}.csv"
                out_dat = f"/home/gulce/Downloads/thesis/data/fair/tmp/consensus_dag_{dag_index}_{timestamp}.dat"
                out_jkl = f"/home/gulce/Downloads/thesis/data/fair/tmp/consensus_dag_{dag_index}_{timestamp}.jkl"

                import data_preparation
                df_data.to_csv(out_csv, index=False)
                # Or use your own method that writes the second line, etc.
                data_preparation.save_data(df_data, out_csv, out_dat)
                data_preparation.compute_bdeu_scores(out_dat, out_jkl)

                # [5] Parse .jkl file, create sumu Data, etc.
                import sumu
                import heuristics
                parsed_scores = data_io.parse_gobnilp_jkl(out_jkl)
                scores = heuristics.GobnilpScores(parsed_scores)
                n = scores.n

                # sumu requires a Data object
                df_sumu = pd.read_csv(out_csv, skiprows=[1])  # skip second line if needed
                mydata = sumu.Data(df_sumu.values)

                # [6] Prepare candidate algorithms
                from sumu.candidates import candidate_parent_algorithm as cpa
                candidate_algos = {
                    "top": (cpa["top"], {"scores": scores, "n": n}),
                    # "opt": (cpa["opt"], {"scores": scores, "n": n}),
                    "mb": (cpa["mb"], {"data": mydata, "fill": "random"}),
                    "pc": (cpa["pc"], {"data": mydata, "fill": "random"}),
                    "ges": (cpa["ges"], {"scores": scores, "data": mydata, "fill": "top"}),
                    "greedy": (cpa["greedy"], {"scores": scores}),
                    "greedy-lite": (cpa["greedy-lite"], {"scores": scores}),
                    "back-forth": (cpa["back-forth"], {"scores": scores, "data": scores.data}),
                    "beam": (heuristics.beam_bdeu, {"scores": scores, "beam_size": 5}),
                    "marginal_bdeu_parents": (heuristics.marginal_bdeu_parents, {"scores": scores, "n": n}),
                    "voting_bdeu_parents": (heuristics.bdeu_score_based_voting, {"scores": scores}),
                    "synergy": (heuristics.synergy_based_parent_selection, {"scores": scores}),
                    "stability": (heuristics.stability_bdeu, {"scores": scores, "data": mydata}),
                    "post": (heuristics.maximize_true_graph_posterior, {"scores": scores}),
                }

                # Convert true_dag to a consistent dict-of-tuples for coverage
                # If your true_dag is already { node : [parents...] }, do:
                true_parents = {}
                for node, parlist in true_dag.items():
                    true_parents[node] = tuple(parlist)

                import time

                # Evaluate each algo at K=1..9
                for algo_name, (algo_func, algo_kwargs) in candidate_algos.items():
                    for K in range(1, scores.n):
                        start_time = time.time()
                        candidate_parents = None
                        try:
                            tmp_result = algo_func(K, **algo_kwargs)
                            # Some algorithms might return (candidate_parents, something_else)
                            if isinstance(tmp_result, tuple) and len(tmp_result) >= 1:
                                candidate_parents = tmp_result[0]
                            else:
                                candidate_parents = tmp_result
                        except Exception as e:
                            logging.warning(f"[Algo={algo_name}, K={K}] error: {e}")
                            # You might store a row with Nones:
                            all_results.append((
                                dag_index, size, 1,
                                algo_name, K,
                                None, None, None, None, None, None
                            ))
                            continue

                        elapsed = time.time() - start_time
                        # If you have a time cutoff:
                        if elapsed > 200:
                            logging.warning(f"[Algo={algo_name}, K={K}] SKIPPED, took {elapsed:.1f}s > 200s")
                            # store skip
                            all_results.append((
                                dag_index, size, 1,
                                algo_name, K,
                                None, None, None, None, None, None
                            ))
                            break

                        # Compute coverage metrics
                        exact_cov = exact_coverage(true_parents, candidate_parents)
                        avg_cov = average_parent_coverage(true_parents, candidate_parents)
                        precision, recall, f1 = precision_recall_f1(true_parents, candidate_parents)
                        shd = structural_hamming_distance(true_parents, candidate_parents)
                        rank_cov = rank_coverage(true_parents, candidate_parents)

                        # Append to big list
                        all_results.append((
                            dag_index, size, 1,
                            algo_name, K,
                            exact_cov,
                            avg_cov,
                            precision,
                            recall,
                            f1,
                            shd
                            # you could also include rank_cov if you want
                        ))
    # ------------------------------------------------------------------
    # [7] After finishing *all DAGs*, group by (Algorithm, K) 
    #     and compute the average across DAGs (and possibly across replicates).
    df_cols = [
        "dag_index", "sample_size", "replicate",
        "Algorithm", "K",
        "Exact Coverage", "Average Parent Coverage",
        "Precision", "Recall", "F1", "SHD"
    ]
    df_all = pd.DataFrame(all_results, columns=df_cols)




    # If you want a separate average per sample_size, do:
    df_agg = (
        df_all.groupby(["Algorithm", "K", "sample_size"], dropna=True)
              [["Exact Coverage", "Average Parent Coverage",
                "Precision", "Recall", "F1", "SHD"]]
              .mean()
              .reset_index()
    )

    # [8] Save the aggregated results
    out_log_csv = f"/home/gulce/Downloads/thesis/data/consesus/consensus_log_{timestamp}.csv"
    df_agg.to_csv(out_log_csv, index=False)
    print(f"[INFO] Averaged results saved to {out_log_csv}")

if __name__ == "__main__":
    main()




# mybnexp/heuristics.py

import numpy as np
import logging

import sumu
from sumu.candidates import candidate_parent_algorithm as cpa
class Data:
    """Simple container for the number of variables `n`."""
    def __init__(self, n):
        self.n = n

class GobnilpScores:
    """
    A Scores-like class that wraps the output of parse_gobnilp_jkl 
    for use with sumu's candidate parent algorithms.
    """
    def __init__(self, parsed_scores):
        """
        Args:
            parsed_scores (dict): 
                A dict of { node: [ (score, (parents...)), ... ], ... }
        """
        self.n = max(parsed_scores.keys()) + 1
        self.data = Data(self.n)
        print("-----------------------------------",flush=True)
        
        # Store local scores in { node: {parents_tuple: score} }
        self.scores = {}
        for node, sp_list in parsed_scores.items():
            self.scores[node] = {}
            for (score, parents) in sp_list:
                parents_sorted = tuple(sorted(parents))
                self.scores[node][parents_sorted] = score

        # If you do not have a known maximum parent set size, keep this -1
        self.maxid = -1

    def local(self, v, parents):
        """
        Sumu calls 'scores.local(...)' in the candidate generation.
        So, we must provide this method name exactly.
        """
        p_sorted = tuple(sorted(parents))
        return self.scores[v].get(p_sorted, float("-inf"))
    
    def _local(self, v, parents):
        """
        Sumu calls 'scores.local(...)' in the candidate generation.
        So, we must provide this method name exactly.
        """
        p_sorted = tuple(sorted(parents))
        return self.scores[v].get(p_sorted, float("-inf"))

    def all_candidate_restricted_scores(self, C):
        import numpy as np
        print("-----------------------------------",flush=True)
        V = len(C)
        # Compute the number of subsets for each node.
        subset_counts = [1 << len(C[i]) for i in range(V)]
        max_subset_count = max(subset_counts)
        arr = np.full((V, max_subset_count), float("-inf"), dtype=float)
        
        for i in range(V):
            # Ensure the candidate parents for node i are sorted.
            sorted_parents = sorted(C[i])
            subset_count = 1 << len(sorted_parents)
            for m in range(subset_count):
                # Reverse the bit order so that the leftmost (first) element of the sorted list
                # corresponds to the most significant bit.
                parents_tuple = tuple(
                    sorted_parents[k] for k in range(len(sorted_parents))
                    if (m & (1 << (len(sorted_parents) - 1 - k)))
                )
                # Look up the score; if missing, use -inf.
                sc = self.scores[i].get(parents_tuple, float("-inf"))
                arr[i, m] = sc
        return arr


    def sum(self, v, U, T):

        from itertools import combinations

        # Compute the union of sets U and T
        combined_parents = U | T  # This will create a single set containing all elements from U and T

        # Determine the maximum number of parents (no limit in this case)
        max_parents = len(combined_parents)

        total_score = float("-inf")

        # Iterate over all possible parent sets from the union of U and T
        for k in range(max_parents + 1):
            for parent_set in combinations(combined_parents, k):
                score = self.local(v, parent_set)
                total_score = np.logaddexp(total_score, score)

        return total_score

    def clear_cache(self):
        """If your scoring logic uses caching, clear it here; otherwise do nothing."""
        pass
        """_summary_
        """    

    def filter_parent_sets_by_size(self, k):
            """
            Filter the local scores to only include candidate parent sets with exactly k parents.
            """
            for node in self.scores:
                self.scores[node] = {
                    parents: score
                    for parents, score in self.scores[node].items()
                    if len(parents) == k
                }


    def clear_cache(self):
        """If your scoring logic uses caching, clear it here; otherwise do nothing."""
        pass
    


def beam_bdeu(K, scores, beam_size=5, seed=None):
    """
    Custom beam search to pick exactly K parents for each node, maximizing local BDeu (scores.local).

    Returns: dict: node -> tuple_of_parents
    """
    if seed is not None:
        np.random.seed(seed)
    
    n = scores.n

    def score(v, pset):
        # sumu's scores.local expects a list or np.array
        return scores.local(v, np.array(list(pset)))

    candidate_parents = {}
    for v in range(n):
        # all possible parents except v
        possible_parents = [u for u in range(n) if u != v]

        if len(possible_parents) < K:
            logging.warning(f"Node {v}: cannot pick K={K} parents out of {len(possible_parents)} possible!")
            # fallback or raise an error
            raise ValueError(f"Node {v} has fewer than K possible parents.")

        # Start beam with empty set
        beam = [(score(v, []), frozenset())]

        for _ in range(K):
            new_level = []
            for (old_score, pset) in beam:
                for cand in possible_parents:
                    if cand not in pset:
                        new_pset = set(pset)
                        new_pset.add(cand)
                        new_score = score(v, new_pset)
                        new_level.append((new_score, frozenset(new_pset)))
            
            new_level.sort(key=lambda x: x[0], reverse=True)
            beam = new_level[:beam_size]
        
        # best among subsets of size K
        best_score, best_pars = max(beam, key=lambda x: x[0])
        candidate_parents[v] = tuple(sorted(best_pars))

    return candidate_parents


def get_candidate_parents(algo_name, K, scores, data=None, fill="top", **kwargs):
    """
    Return candidate parents for each node, depending on `algo_name`.

    If algo_name == "beam", uses custom beam_bdeu.
    Otherwise uses sumu's built-in candidate_parent_algorithm (cpa).

    Returns: dict: {node: parent_set or parent_tuple}
    """
    if algo_name == "beam":
        return beam_bdeu(K=K, scores=scores, **kwargs)
    else:
        # use sumu's cpa
        algo = cpa[algo_name]
        # Some cpa methods require data; some do not. We unify the call:
        # By default, let's pass both scores and data if they exist:
        return algo(K, scores=scores, data=data, fill=fill)


import numpy as np
from itertools import combinations
from math import log, exp

def logsumexp(x):
    """Stable log-sum-exp."""
    max_x = np.max(x)
    return max_x + np.log(np.sum(np.exp(x - max_x)))

def marginal_bdeu_parents(K, **kwargs):
    """
    Select candidate parents by marginal BDeu posterior.

    For each node v, we compute for each potential parent u:
        P(u in ParentSet(v)) = sum_{S : u in S} exp( BDeu(v,S) )
    normalized by the sum of exp( BDeu(v,S) ) over *all* S.

    Then pick top-K parents of v by this marginal probability.

    Parameters
    ----------
    K : int
        Maximum number of parents to keep for each node.
    kwargs : dict
        Must contain 'scores', which is a GobnilpScores-like object with:
            - scores.scores[v]: dict {parents_tuple: log_bdeu_score}
            - a 'local(v, parents)' method
            - an integer 'n' or 'scores.data.n' for the number of variables
        Optionally can contain 'n', if not inferred from the scores object.
        'fill' can be 'none', 'top', or 'random'.

    Returns
    -------
    C : dict
        Dictionary of the form { v : tuple_of_parents }.
    """
    # 1) Get scores object
    scores = kwargs.get("scores", None)
    if scores is None:
        raise ValueError("marginal_bdeu_parents requires 'scores' in kwargs.")
    
    fill_method = kwargs.get("fill", "none")

    # 2) Determine number of variables 'n'
    n = kwargs.get("n", None)
    if n is None:
        # Attempt to retrieve n from the scores object
        if hasattr(scores, "n"):
            n = scores.n
        elif hasattr(scores, "data") and hasattr(scores.data, "n"):
            n = scores.data.n
        else:
            raise ValueError("Cannot find 'n' from either kwargs or the scores object.")

    # 3) Dictionary to hold the final candidate parents
    C = {}

    # 4) Loop over each node v
    for v in range(n):

        # Instead of 'if v not in scores:', use 'scores.scores'
        if v not in scores.scores:
            # If no entry, means no known subsets => no candidates
            C[v] = ()
            continue

        # v_subsets: dict mapping { (p1, p2, ...): log_BDeu, ... }
        v_subsets = scores.scores[v]
        if not v_subsets:
            # empty => no parents
            C[v] = ()
            continue

        # -- Compute the log normalizer: logsumexp over all subsets' scores
        all_log_scores = list(v_subsets.values())
        normalizer = logsumexp(all_log_scores)  # log Z

        # -- For each potential parent u != v, gather log-scores of subsets that contain u
        logp_u = {}
        for u in range(n):
            if u == v:
                continue
            log_values = []
            for parent_set, logscore in v_subsets.items():
                if u in parent_set:
                    log_values.append(logscore)
            if len(log_values) == 0:
                logp_u[u] = float("-inf")  # u never appears in any subset
            else:
                logp_u[u] = logsumexp(log_values)

        # -- Sort parents by logp_u[u] descending (equivalent to marginal posterior)
        candidate_list = [(u, lv) for (u, lv) in logp_u.items()]
        candidate_list.sort(key=lambda x: x[1], reverse=True)

        # -- Pick top-K
        chosen = [u for (u, lv) in candidate_list[:K]]
        C[v] = tuple(sorted(chosen))

    # 5) If fill_method is 'top' or 'random', optionally fill/prune to K exactly
    if fill_method in ["top", "random"]:
        C = _adjust_number_candidates(K, C, method=fill_method, scores=scores)

    return C


def _adjust_number_candidates(K, C, method, scores=None):
    """Adjust the number of candidate parents to exactly K for each node."""
    assert method in ['random', 'top'], "method must be 'random' or 'top'"
    
    for v in C:
        current_parents = list(C[v])
        parent_count = len(current_parents)

        if parent_count < K:
            # Need to ADD parents
            needed = K - parent_count
            add_from = [node for node in range(len(C)) if node != v and node not in current_parents]

            if method == 'random':
                chosen = np.random.choice(add_from, needed, replace=False)
                current_parents += chosen.tolist()

            elif method == 'top' and scores is not None:
                scored_list = []
                for parent_candidate in add_from:
                    score_val = scores.local(v, np.array([parent_candidate]))
                    scored_list.append((parent_candidate, score_val))
                scored_list.sort(key=lambda x: x[1], reverse=True)
                best = [p for (p, s_val) in scored_list[:needed]]
                current_parents += best

        elif parent_count > K:
            # Need to PRUNE parents
            excess = parent_count - K
            if method == 'random':
                chosen_to_keep = np.random.choice(current_parents, K, replace=False)
                current_parents = list(chosen_to_keep)
            elif method == 'top' and scores is not None:
                scored_list = []
                for p in current_parents:
                    score_val = scores.local(v, np.array([p]))
                    scored_list.append((p, score_val))
                scored_list.sort(key=lambda x: x[1], reverse=True)
                current_parents = [p for (p, s_val) in scored_list[:K]]

        # Update
        C[v] = tuple(sorted(set(current_parents)))

    return C
import numpy as np
from itertools import combinations

def score_improvement(v, u, scores, potential_parents):
    """
    Calculate the score improvement for adding a parent u to the current parent set of node v.
    
    Parameters:
    v: Node for which we are selecting parents.
    u: Potential parent to be added.
    scores: GobnilpScores-like object containing the BDeu scores.
    potential_parents: Current set of parents for node v.
    
    Returns:
    score_improvement: The improvement in the BDeu score when adding parent u.
    """
    # Get the current score of the node v with its current parent set
    current_score = scores.local(v, np.array(potential_parents))
    
    # Add the potential parent u to the current set of parents
    new_parents = tuple(sorted(potential_parents + (u,)))
    new_score = scores.local(v, np.array(new_parents))
    
    # The improvement in the BDeu score is the difference
    score_improvement = new_score - current_score
    return score_improvement

import numpy as np
import logging

import sumu
from sumu.candidates import candidate_parent_algorithm as cpa


########################################
# 1) Fix the Data class so "object of type 'Data' has no len()" is resolved:
########################################
class Data:
    """Simple container for the number of variables `n`."""
    def __init__(self, n):
        self.n = n

    def __len__(self):
        """
        Some code (like stability selection) tries to do `len(data)`.
        Defining this lets that code run without TypeError.
        But be aware that if code tries `data[idx, :]`, 
        you still need __getitem__ for real row-based sampling.
        """
        return self.n
    
    def __len__(self):
        """
        If needed, let's define __len__ so that code 
        that calls len(data) won't crash. 
        """
        return self.n


class GobnilpScores:
    """
    A Scores-like class that wraps the output of parse_gobnilp_jkl 
    for use with sumu's candidate parent algorithms.
    """
    def __init__(self, parsed_scores):
        """
        Args:
            parsed_scores (dict): 
                A dict of { node: [ (score, (parents...)), ... ], ... }
        """
        self.n = max(parsed_scores.keys()) + 1
        self.data = Data(self.n)
        
        # Store local scores in { node: {parents_tuple: score} }
        self.scores = {}
        for node, sp_list in parsed_scores.items():
            self.scores[node] = {}
            for (score, parents) in sp_list:
                parents_sorted = tuple(sorted(parents))
                self.scores[node][parents_sorted] = score

        # If you do not have a known maximum parent set size, keep this -1
        self.maxid = -1

    def local(self, v, parents):
        """
        Sumu calls 'scores.local(...)' in the candidate generation.
        So, we must provide this method name exactly.
        """
        p_sorted = tuple(sorted(parents))
        return self.scores[v].get(p_sorted, float("-inf"))
    
    def _local(self, v, parents):
        """
        Same as 'local' above; sometimes sumu calls _local(...) internally.
        """
        p_sorted = tuple(sorted(parents))
        return self.scores[v].get(p_sorted, float("-inf"))

    def all_candidate_restricted_scores(self, C):
        import numpy as np
        V = len(C)
        # Compute the number of subsets for each node.
        subset_counts = [1 << len(C[i]) for i in range(V)]
        max_subset_count = max(subset_counts)
        arr = np.full((V, max_subset_count), float("-inf"), dtype=float)
        
        for i in range(V):
            # Sort the candidate parents for node i.
            sorted_parents = sorted(C[i])
            subset_count = 1 << len(sorted_parents)
            for m in range(subset_count):
                # Use natural bit order: bit k corresponds to sorted_parents[k]
                parents_tuple = tuple(
                    sorted_parents[k]
                    for k in range(len(sorted_parents))
                    if (m & (1 << k))
                )
                # Look up the score; if missing, use -inf.
                sc = self.scores[i].get(parents_tuple, float("-inf"))
                arr[i, m] = sc
        return arr


    def sum(self, v, U, T):
        """
        Example method that sums log-scores over subsets from the union of U and T,
        for node v.
        """
        from itertools import combinations
        combined_parents = U | T
        max_parents = len(combined_parents)
        total_score = float("-inf")

        for k in range(max_parents + 1):
            for parent_set in combinations(combined_parents, k):
                score = self.local(v, parent_set)
                total_score = np.logaddexp(total_score, score)
        return total_score

    def clear_cache(self):
        """If your scoring logic uses caching, clear it here; otherwise do nothing."""
        pass


########################################
# 2) "beam_bdeu" is fine, no changes needed
########################################
def beam_bdeu(K, scores, beam_size=5, seed=None):
    if seed is not None:
        np.random.seed(seed)
    
    n = scores.n

    def score(v, pset):
        return scores.local(v, np.array(list(pset)))

    candidate_parents = {}
    for v in range(n):
        possible_parents = [u for u in range(n) if u != v]

        if len(possible_parents) < K:
            logging.warning(f"Node {v}: cannot pick K={K} parents out of {len(possible_parents)} possible!")
            raise ValueError(f"Node {v} has fewer than K possible parents.")

        beam = [(score(v, []), frozenset())]

        for _ in range(K):
            new_level = []
            for (old_score, pset) in beam:
                for cand in possible_parents:
                    if cand not in pset:
                        new_pset = set(pset)
                        new_pset.add(cand)
                        new_score = score(v, new_pset)
                        new_level.append((new_score, frozenset(new_pset)))
            
            new_level.sort(key=lambda x: x[0], reverse=True)
            beam = new_level[:beam_size]
        
        best_score, best_pars = max(beam, key=lambda x: x[0])
        candidate_parents[v] = tuple(sorted(best_pars))

    return candidate_parents


def get_candidate_parents(algo_name, K, scores, data=None, fill="top", **kwargs):
    """
    Return candidate parents for each node, depending on `algo_name`.
    If algo_name == "beam", uses custom beam_bdeu.
    Otherwise uses sumu's built-in candidate_parent_algorithm (cpa).
    """
    if algo_name == "beam":
        return beam_bdeu(K=K, scores=scores, **kwargs)
    else:
        # use sumu's cpa
        algo = cpa[algo_name]
        return algo(K, scores=scores, data=data, fill=fill, **kwargs)


########################################
# 3) Marginal BDeu - ensure no "list+tuple" issues or 'v not in scores' issues
########################################
from itertools import combinations
from math import log

def logsumexp(x):
    """Stable log-sum-exp."""
    max_x = np.max(x)
    return max_x + np.log(np.sum(np.exp(x - max_x)))


def marginal_bdeu_parents(K, **kwargs):
    scores = kwargs.get("scores", None)
    if scores is None:
        raise ValueError("marginal_bdeu_parents requires 'scores' in kwargs.")
    
    fill_method = kwargs.get("fill", "none")

    n = kwargs.get("n", None)
    if n is None:
        if hasattr(scores, "n"):
            n = scores.n
        elif hasattr(scores, "data") and hasattr(scores.data, "n"):
            n = scores.data.n
        else:
            raise ValueError("Cannot find 'n' in kwargs or scores object.")

    C = {}

    for v in range(n):
        if v not in scores.scores:
            C[v] = ()
            continue

        v_subsets = scores.scores[v]
        if not v_subsets:
            C[v] = ()
            continue

        all_log_scores = list(v_subsets.values())
        normalizer = logsumexp(all_log_scores)

        logp_u = {}
        for u in range(n):
            if u == v:
                continue
            log_values = []
            for parent_set, logscore in v_subsets.items():
                if u in parent_set:
                    log_values.append(logscore)
            if len(log_values) == 0:
                logp_u[u] = float("-inf")
            else:
                logp_u[u] = logsumexp(log_values) - normalizer

        # sort by logp_u[u] desc
        candidate_list = [(u, lv) for (u, lv) in logp_u.items()]
        candidate_list.sort(key=lambda x: x[1], reverse=True)

        chosen = [u for (u, lv) in candidate_list[:K]]
        C[v] = tuple(sorted(chosen))

    if fill_method in ["top", "random"]:
        C = _adjust_number_candidates(K, C, method=fill_method, scores=scores)

    return C


def _adjust_number_candidates(K, C, method, scores=None):
    assert method in ['random', 'top'], "method must be 'random' or 'top'"
    
    for v in C:
        current_parents = list(C[v])
        parent_count = len(current_parents)

        if parent_count < K:
            needed = K - parent_count
            add_from = [node for node in range(len(C)) if node != v and node not in current_parents]

            if method == 'random':
                chosen = np.random.choice(add_from, needed, replace=False)
                current_parents += chosen.tolist()

            elif method == 'top' and scores is not None:
                scored_list = []
                for parent_candidate in add_from:
                    score_val = scores.local(v, np.array([parent_candidate]))
                    scored_list.append((parent_candidate, score_val))
                scored_list.sort(key=lambda x: x[1], reverse=True)
                best = [p for (p, s_val) in scored_list[:needed]]
                current_parents += best

        elif parent_count > K:
            if method == 'random':
                chosen_to_keep = np.random.choice(current_parents, K, replace=False)
                current_parents = list(chosen_to_keep)
            elif method == 'top' and scores is not None:
                scored_list = []
                for p in current_parents:
                    score_val = scores.local(v, np.array([p]))
                    scored_list.append((p, score_val))
                scored_list.sort(key=lambda x: x[1], reverse=True)
                current_parents = [p for (p, s_val) in scored_list[:K]]

        C[v] = tuple(sorted(set(current_parents)))

    return C


########################################
# 4) BDeu Score-Based Voting (bdeu_score_based_voting) 
#    to handle "list+tuple" fixes
########################################
def score_improvement(v, u, scores, current_set):
    """
    Calculate the improvement from adding parent u to `current_set` for node v.
    """
    current_score = scores.local(v, np.array(current_set))
    new_parents = tuple(sorted(current_set + (u,)))
    new_score = scores.local(v, np.array(new_parents))
    return new_score - current_score

def bdeu_score_based_voting(K, **kwargs):
    scores = kwargs.get("scores", None)
    if scores is None:
        raise ValueError("bdeu_score_based_voting requires 'scores' in kwargs.")
    
    n = kwargs.get("n", None)
    if n is None:
        n = scores.n if hasattr(scores, "n") else scores.data.n
    
    C = {}

    for v in range(n):
        potential_parents = [u for u in range(n) if u != v]
        
        # For each parent, measure improvement
        parent_contributions = {}
        for u in potential_parents:
            # treat the "current_set" as empty for measure
            improvement = score_improvement(v, u, scores, ())
            parent_contributions[u] = improvement
        
        sorted_parents = sorted(parent_contributions, key=parent_contributions.get, reverse=True)
        C[v] = tuple(sorted(sorted_parents[:K]))
    
    return C


########################################
# 5) Synergy-based approach 
#    Accept 'n=None' so no "unexpected keyword argument 'n'" error
########################################
def synergy_for_node(
    v, K, scores, alpha=0.0, fallback=True
):
    """
    Select up to K parents for node v using synergy-based search.
    
    If no positive synergy is found, we stop. If that yields fewer
    than K parents and 'fallback' is True, we pick top singletons
    from the leftover candidates to fill up to K.
    
    v : int
        The node index
    K : int
        Maximum number of parents
    scores : GobnilpScores
        Has .local(v, parents) -> log BDeu
    alpha : float
        Synergy offset factor
    fallback : bool
        Whether to fallback to top singletons if synergy picks fewer than K.
    """
    n = scores.n
    candidates = [u for u in range(n) if u != v]

    # Precompute BDeu(empty) and singletons
    bdeu_empty = scores.local(v, np.array([], dtype=int))
    bdeu_single = {u: scores.local(v, np.array([u])) for u in candidates}

    S = ()  # current parent set
    while len(S) < K:
        best_gain = float("-inf")
        best_u = None

        # current BDeu of S
        bdeu_S = scores.local(v, np.array(S))

        for u in candidates:
            if u in S:
                continue
            # synergy gain = BDeu(S ∪ {u}) - BDeu(S) - alpha*(BDeu({u}) - BDeu({}))
            bdeu_Su = scores.local(v, np.array(S + (u,)))
            gain = bdeu_Su - bdeu_S
            if alpha > 0:
                gain -= alpha * (bdeu_single[u] - bdeu_empty)

            if gain > best_gain:
                best_gain = gain
                best_u = u

        if best_gain <= 0 or best_u is None:
            # no further synergy improvement
            break

        # Otherwise add best_u
        S = tuple(sorted(S + (best_u,)))

    # if synergy yields fewer than K parents and fallback==True,
    # optionally fill from best singletons among leftover candidates
    if fallback and len(S) < K:
        # leftover are the candidates not in S
        leftover = [u for u in candidates if u not in S]
        # sort by single-parent BDeu descending
        leftover.sort(key=lambda u: bdeu_single[u], reverse=True)
        needed = K - len(S)
        # pick top 'needed' leftover
        fill_pars = leftover[:needed]
        S = tuple(sorted(S + tuple(fill_pars)))

    return S


###############################################################################
# Synergy-based parent selection for all nodes
###############################################################################
def synergy_based_parent_selection(K, scores, alpha=0.0, fallback=True):
    """
    For each node v, call synergy_for_node(...) to pick up to K parents.
    
    K : int
        Max parents
    scores : GobnilpScores
        log BDeu data
    alpha : float
        synergy offset
    fallback : bool
        Whether to fill parents with best singletons if synergy picks none.
    """
    n = scores.n
    C = {}
    for v in range(n):
        # If the node has no local scores, we can't pick anything
        if v not in scores.scores or len(scores.scores[v]) == 0:
            C[v] = ()
            continue

        # Otherwise run synergy
        chosen = synergy_for_node(v, K, scores, alpha=alpha, fallback=fallback)
        C[v] = chosen

    return C


########################################
# 6) Example "stability_bdeu" approach
#    We define a minimal fix to allow len(data) by using Data.__len__ above.
########################################

def stability_bdeu(K, scores, data, B=20, threshold=0.5, fill='top'):
    """
    Perform bootstrap-based stability selection for candidate parents 
    using BDeu scores. If the BDeu scores are empty or all -inf, 
    we gracefully return empty sets or fallback.

    Parameters
    ----------
    K : int
        Max number of parents per node.
    scores : GobnilpScores
        BDeu scoring object for the full dataset.
    data : array-like or Data object
        The 'dataset'. If it is just Data(n), there's no real row dimension.
    B : int
        Number of bootstrap samples.
    threshold : float
        Frequency threshold in [0,1].
    fill : str
        'top' or 'random' or 'none', how to fill if fewer than K stable parents.

    Returns
    -------
    C_stable : dict
        { v : tuple_of_parents } with up to K parents.
    """
    n = scores.n
    # freq[v,u] = how many times u was chosen for v
    freq = np.zeros((n, n), dtype=float)

    for _ in range(B):
        # 1) "sample" data. If real data is not provided, 
        #    this simply returns 'data' or does nothing
        sampled_data = bootstrap_data(data)
        # 2) compute BDeu scores from the sampled data
        sampled_scores = scores

        # 3) pick top-K parents in *this bootstrap* for each node
        #    using our fallback-based function:
        C_b = pick_top_k_parents(sampled_scores, K)

        for v, parents in C_b.items():
            for p in parents:
                freq[v, p] += 1

    freq /= B  # convert to frequency

    # Now build stable sets
    C_stable = {}
    for v in range(n):
        stable_pars = [u for u in range(n) if u != v and freq[v, u] >= threshold]
        if len(stable_pars) > K:
            # prune
            stable_pars.sort(key=lambda u: freq[v,u], reverse=True)
            stable_pars = stable_pars[:K]
        elif len(stable_pars) < K and fill=='top':
            # fill
            others = [u for u in range(n) if u != v and u not in stable_pars]
            others.sort(key=lambda x: freq[v,x], reverse=True)
            needed = K - len(stable_pars)
            stable_pars += others[:needed]

        C_stable[v] = tuple(sorted(stable_pars))

    return C_stable

###############################################################################
# 2) Bootstrapping and BDeu scoring stubs with fallback
###############################################################################
def bootstrap_data(data):
    """
    If data is just Data(n), we do no real sampling. 
    For a real dataset, you would do e.g.:

    idxs = np.random.choice(len(data), len(data), replace=True)
    return data[idxs, :]
    """
    return data



###############################################################################
# 3) pick_top_k_parents with fallback to avoid empty sets
###############################################################################
def pick_top_k_parents(scores, K):
    """
    For each node v, pick top-K parents from scores.
    If the node has no singletons or all -inf, fallback to empty set.
    """
    C_b = {}
    n = scores.n

    for v in range(n):
        # If scores is missing or empty, fallback to ()
        if v not in scores.scores or not scores.scores[v]:
            C_b[v] = ()
            continue

        # Collect single-parent subsets
        singletons = {}
        for pset, val in scores.scores[v].items():
            if len(pset) == 1:
                singletons[pset[0]] = val

        # If singletons is empty, fallback to the best available set
        # For example, pick the highest scoring multi-parent if it exists:
        if not singletons:
            # fallback to the best subset overall
            best_sub = None
            best_val = float("-inf")
            for pset, val in scores.scores[v].items():
                if val > best_val:
                    best_val = val
                    best_sub = pset
            if best_sub is None or best_val == float("-inf"):
                # no valid subsets => empty
                C_b[v] = ()
            else:
                # If best_sub has more than K parents, we still prune
                if len(best_sub) <= K:
                    C_b[v] = tuple(sorted(best_sub))
                else:
                    # arbitrary prune if best_sub is bigger than K
                    # e.g. pick top K parents from best_sub
                    C_b[v] = tuple(sorted(best_sub)[:K])
            continue

        # Otherwise pick top K singletons
        sorted_singles = sorted(singletons.items(), key=lambda x: x[1], reverse=True)
        top_k = [p for (p, sc) in sorted_singles[:K]]
        C_b[v] = tuple(sorted(top_k))

    return C_b
import gurobipy as gp
from gurobipy import GRB
import itertools
import numpy as np


class BayesianNetwork:
    def __init__(self, structure, scores):
        """
        structure: dict of node -> tuple/set of parent nodes
        scores: GobnilpScores (with real BDeu up to 3 parents)
        """
        self.structure = structure
        self.scores = scores

    def compute_posterior(self):
        """
        Sums up real local scores.  If any node has more than 3 parents,
        the real GobnilpScores.local(...) might be -inf => -inf total.
        So this step is only valid if you do indeed have those bigger sets scored,
        or you rely on the same approximation again here.
        """
        total_score = 0.0
        for node, pars in self.structure.items():
            val = self.scores.local(node, tuple(sorted(pars)))
            if val == float('-inf'):
                return float('-inf')
            total_score += val
        return total_score

def approximate_score(node, parents, scores):
    """
    Approximate local score using existing up-to-3-parent data.
    If parents has size > 3, pick best 3-subset's real local score as a stand-in.
    """
    if len(parents) <= 3:
        return scores.local(node, tuple(sorted(parents)))

    best_sub_score = -float("inf")
    for sub in itertools.combinations(parents, 3):
        sc = scores.local(node, tuple(sorted(sub)))
        if sc > best_sub_score:
            best_sub_score = sc
    return best_sub_score

def compute_approx_score_for_candidate(node, parents, scores):
    """
    Use an approximate scheme to get a 'score' for sets possibly > 3 parents.
    """
    return approximate_local_score(node, parents, scores)

def approximate_local_score(node, parents, scores):
    """
    Approximate local score for sets with > 3 parents by averaging
    the real GobnilpScores for all 3-subsets.

    If len(parents) <= 3, just return the real local score.
    """
    from math import isfinite
    
    sz = len(parents)
    # If 3 or fewer, return the real score
    if sz <= 3:
        return scores.local(node, tuple(sorted(parents)))

    # For > 3, let's compute the average across all 3-subsets
    all_3_subsets = list(itertools.combinations(parents, 3))
    scores_3sub = []
    for sub in all_3_subsets:
        sc_sub = scores.local(node, tuple(sorted(sub)))
        if isfinite(sc_sub):
            scores_3sub.append(sc_sub)
        else:
            # If any 3-subset is -inf, we can either skip it or treat it as -inf
            # (which will drag the average down heavily).
            scores_3sub.append(float('-inf'))

    if not scores_3sub:
        # If they are all -inf, the approximate score is -inf
        return float('-inf')
    
    # Return the average or the max or any combination
    return sum(scores_3sub)/len(scores_3sub)


def maximize_true_graph_posterior( K,scores):
    """
    Column generation for exactly K parents per node,
    but if K>3, we approximate the local score of a bigger set by
    the best 3-parent subset (or another scheme).

    Returns: a dict of node -> chosen parent set (size exactly K).
    """
    n=scores.n
    master = gp.Model("MasterProblem")
    master.Params.OutputFlag = 0

    x_vars = {}
    columns = {}

    # 1) Build candidate columns for each node: all subsets of size K
    for i in range(n):
        columns[i] = []
        possible_parents = [p for p in range(n) if p != i]

        for cand in itertools.combinations(possible_parents, K):
            sc = compute_approx_score_for_candidate(i, cand, scores)
            if np.isfinite(sc):
                var = master.addVar(vtype=GRB.CONTINUOUS, lb=0, ub=1,
                                    name=f"x_{i}_{cand}")
                x_vars[(i, cand)] = var
                columns[i].append(cand)

    master.update()

    # 2) Constraints: each node picks exactly one K-parent set
    constrs = {}
    for i in range(n):
        if len(columns[i]) == 0:
            raise ValueError(
                f"No feasible K={K} parent sets for node {i} with the approximate scoring."
            )
        constrs[i] = master.addConstr(
            gp.quicksum(x_vars[(i, c)] for c in columns[i]) == 1,
            name=f"node_{i}"
        )
    master.update()

    # 3) Objective: sum of approximate scores
    obj_expr = gp.LinExpr()
    for i in range(n):
        for cand in columns[i]:
            sc = compute_approx_score_for_candidate(i, cand, scores)
            obj_expr.addTerms(sc, x_vars[(i, cand)])
    master.setObjective(obj_expr, GRB.MAXIMIZE)
    master.update()

    # 4) Column generation loop
    improved = True
    iteration = 0
    while improved:
        iteration += 1
        master.optimize()
        if master.status != GRB.OPTIMAL:
            print(f"Master problem not optimal at iteration {iteration}; stopping.")
            break

        duals = {i: constrs[i].Pi for i in range(n)}
        improved = False

        # Pricing: find columns (K-subsets) with positive reduced cost
        for i in range(n):
            best_rc = -float('inf')
            best_cand = None
            possible_parents = [p for p in range(n) if p != i]
            for cand in itertools.combinations(possible_parents, K):
                if cand in columns[i]:
                    continue
                sc = compute_approx_score_for_candidate(i, cand, scores)
                if not np.isfinite(sc):
                    continue
                rc = sc - duals[i]
                if rc > best_rc:
                    best_rc = rc
                    best_cand = cand

            # Add any new column with significantly positive reduced cost
            if best_cand is not None and best_rc > 1e-8:
                improved = True
                columns[i].append(best_cand)
                var = master.addVar(vtype=GRB.CONTINUOUS, lb=0, ub=1,
                                    name=f"x_{i}_{best_cand}")
                x_vars[(i, best_cand)] = var
                master.chgCoeff(constrs[i], var, 1.0)
                master.setObjective(master.getObjective() + best_rc * var)
                master.update()

    # 5) Extract solution
    solution = {}
    for i in range(n):
        best_val = -1.0
        best_cand = None
        for cand in columns[i]:
            val = x_vars[(i, cand)].X
            if val > best_val:
                best_val = val
                best_cand = cand
        solution[i] = tuple(sorted(best_cand)) if best_cand else ()
        
    print(f"Column generation done after {iteration} iteration(s).")
    return solution
# import numpy as np
# from itertools import combinations
# from scipy.optimize import linprog

# # --- Dummy parser and classes for demonstration purposes ---

# import data_io
# import heuristics
# # --- ILP with Cutting Planes ---
# class ILPwithCuttingPlanes:
#     def __init__(self, scores):
#         self.scores = scores
#         self.n_nodes = scores.n
#         self.local_scores = scores.local_scores

#     def formulate_ilp(self):
#         """
#         Formulate the ILP by creating an objective vector (c) and
#         equality constraints (A_eq and b_eq) so that exactly one candidate
#         parent set is selected per node.
#         """
#         n = self.n_nodes
#         c = []
#         for node in range(n):
#             # Extend c with the score for each candidate parent set for this node
#             c.extend(self.local_scores[node].values())
#         # Each node must have exactly one selected parent set.
#         A_eq = np.zeros((n, len(c)))
#         b_eq = np.ones(n)
#         idx = 0
#         for node in range(n):
#             for parent_set in self.local_scores[node].keys():
#                 A_eq[node, idx] = 1
#                 idx += 1
#         return c, A_eq, b_eq

#     def solve_ilp(self, c, A_eq, b_eq):
#         """
#         Solve the ILP using scipy's linprog.
#         (Note: This performs a linear relaxation; in practice you might
#         want to use a dedicated MILP solver such as Gurobi or CPLEX.)
#         """
#         result = linprog(c=-np.array(c), A_eq=A_eq, b_eq=b_eq,
#                          bounds=[(0, 1)] * len(c), method='highs')
#         if result.success:
#             return result.x
#         else:
#             return None

#     def add_cutting_plane(self, A_eq, b_eq, violated_constraint):
#         """
#         Add a cutting plane constraint.
#         violated_constraint should be a tuple (new_row, rhs).
#         """
#         new_constraint = violated_constraint
#         A_eq = np.vstack([A_eq, new_constraint[0]])
#         b_eq = np.append(b_eq, new_constraint[1])
#         return A_eq, b_eq

#     def check_violated_constraints(self, solution):
#         """
#         Placeholder for checking if any domain-specific constraints are violated.
#         Implement logic here to detect violations based on your criteria.
#         For this example, we assume no violations.
#         """
#         return None

#     def solve_with_cutting_planes(self, max_iter=10):
#         """
#         Solve the ILP, iteratively adding cutting planes if any constraint
#         is violated.
#         """
#         c, A_eq, b_eq = self.formulate_ilp()
#         solution = self.solve_ilp(c, A_eq, b_eq)
#         if solution is None:
#             print("Initial ILP solution failed")
#             return None
#         for _ in range(max_iter):
#             violated_constraint = self.check_violated_constraints(solution)
#             if violated_constraint:
#                 A_eq, b_eq = self.add_cutting_plane(A_eq, b_eq, violated_constraint)
#                 solution = self.solve_ilp(c, A_eq, b_eq)
#                 if solution is None:
#                     print("ILP solution failed after cutting plane addition")
#                     return None
#             else:
#                 return solution
#         return solution

# def interpret_solution(solution, scores):
#     """
#     Convert the flattened solution vector back into a configuration
#     mapping each node to its selected candidate parent set.
#     """
#     parent_set_config = []
#     idx = 0
#     for node in range(scores.n):
#         num_candidates = len(scores.local_scores[node])
#         for i in range(num_candidates):
#             if solution[idx + i] == 1:
#                 # Retrieve the candidate parent set (by order of keys)
#                 candidate = list(scores.local_scores[node].keys())[i]
#                 parent_set_config.append((node, candidate))
#         idx += num_candidates
#     return parent_set_config

# def filter_parent_sets_by_size(scores, k):
#     for node in scores.local_scores:
#         scores.local_scores[node] = {
#             parents: score
#             for parents, score in scores.local_scores[node].items()
#             if len(parents) == k
#         }

# # --- Main Execution ---
# if __name__ == '__main__':
#     # Replace with your actual parser if available:
#     parsed_scores = data_io.parse_gobnilp_jkl('/home/gulce/Downloads/thesis/data/asia/asia_scores.jkl')
#     scores = heuristics.GobnilpScores(parsed_scores)
    
#     # Restrict candidate parent sets to exactly k parents.
#     k = 2 # Change k to the desired number of parents.

    
#     print("Filtered local_scores (only parent sets of size {}):".format(k))
#     for node, candidates in scores.local_scores.items():
#         print("Node {}: {}".format(node, candidates))
    
#     # Create and solve the ILP with cutting planes.
#     ilp_solver = ILPwithCuttingPlanes(scores)
#     best_solution = ilp_solver.solve_with_cutting_planes()
    
#     if best_solution is not None:
#         print("\nBest parent set configuration (solution vector):")
#         print(best_solution)
#         config = interpret_solution(best_solution, scores)
#         print("\nInterpreted configuration:")
#         for node, parents in config:
#             print(f"Node {node} selected parent set: {parents}")
#     else:
#         print("No feasible solution found.")
import gurobipy as gp
from gurobipy import GRB
import itertools
import numpy as np

class BayesianNetwork:
    def __init__(self, structure, scores):
        """
        structure: dict of node -> tuple/set of parent nodes
        scores: GobnilpScores (with real BDeu up to 3 parents)
        """
        self.structure = structure
        self.scores = scores

    def compute_posterior(self):
        """
        Sums up real local scores.  If any node has more than 3 parents,
        the real GobnilpScores.local(...) might be -inf => -inf total.
        So this step is only valid if you do indeed have those bigger sets scored,
        or you rely on the same approximation again here.
        """
        total_score = 0.0
        for node, pars in self.structure.items():
            val = self.scores.local(node, tuple(sorted(pars)))
            if val == float('-inf'):
                return float('-inf')
            total_score += val
        return total_score

def approximate_local_score(node, parents, local_scores):
    """
    Approximate local score using existing up-to-3-parent data.
    If parents has size > 3, pick best 3-subset's real local score as a stand-in.
    """
    if len(parents) <= 3:
        return local_scores.local(node, tuple(sorted(parents)))

    best_sub_score = -float("inf")
    for sub in itertools.combinations(parents, 3):
        sc = local_scores.local(node, tuple(sorted(sub)))
        if sc > best_sub_score:
            best_sub_score = sc
    return best_sub_score

def maximize_true_graph_posterior(local_scores, n, K):
    """
    Column generation for exactly K parents per node,
    but if K>3, we approximate the local score of a bigger set by
    the best 3-parent subset (or another scheme).

    Returns: a dict of node -> chosen parent set (size exactly K).
    """
    master = gp.Model("MasterProblem")
    master.Params.OutputFlag = 0

    x_vars = {}
    columns = {}

    # Step 1: build candidate columns of size K
    for i in range(n):
        columns[i] = []
        possible_parents = [p for p in range(n) if p != i]

        # All combinations of size K
        for cand in itertools.combinations(possible_parents, K):
            sc = approximate_local_score(i, cand, local_scores)
            # If the approximate score is not -inf or NaN, keep it
            if np.isfinite(sc):
                var = master.addVar(vtype=GRB.CONTINUOUS, lb=0, ub=1,
                                    name=f"x_{i}_{cand}")
                x_vars[(i, cand)] = var
                columns[i].append(cand)

    master.update()

    # Step 2: constraints
    constrs = {}
    for i in range(n):
        if not columns[i]:
            raise ValueError(f"No feasible K={K} parent sets (even approximate) for node {i}.")
        constrs[i] = master.addConstr(
            gp.quicksum(x_vars[(i, c)] for c in columns[i]) == 1,
            name=f"node_{i}"
        )
    master.update()

    # Step 3: objective
    obj_expr = gp.LinExpr()
    for i in range(n):
        for cand in columns[i]:
            sc = approximate_local_score(i, cand, local_scores)
            obj_expr.addTerms(sc, x_vars[(i, cand)])
    master.setObjective(obj_expr, GRB.MAXIMIZE)
    master.update()

    # Step 4: Column generation loop
    improved = True
    iteration = 0
    while improved:
        iteration += 1
        master.optimize()
        if master.status != GRB.OPTIMAL:
            print(f"Master not solved to optimality at iteration {iteration}. Stopping.")
            break

        # Dual values
        duals = {i: constrs[i].Pi for i in range(n)}
        improved = False

        # Pricing
        for i in range(n):
            best_rc = -float('inf')
            best_cand = None
            for cand in itertools.combinations([p for p in range(n) if p != i], K):
                if cand in columns[i]:
                    continue
                sc = approximate_local_score(i, cand, local_scores)
                if not np.isfinite(sc):
                    continue
                rc = sc - duals[i]
                if rc > best_rc:
                    best_rc = rc
                    best_cand = cand

            if best_cand is not None and best_rc > 1e-8:
                improved = True
                columns[i].append(best_cand)
                var = master.addVar(vtype=GRB.CONTINUOUS, lb=0, ub=1,
                                    name=f"x_{i}_{best_cand}")
                x_vars[(i, best_cand)] = var
                master.chgCoeff(constrs[i], var, 1.0)
                # We can add (sc) * var or directly (rc) * var + duals[i]*var, etc.
                # But simpler is to just do sc * var:
                master.setObjective(master.getObjective() + rc * var)
                master.update()

    # Step 5: Extract solution
    solution = {}
    for i in range(n):
        best_val = -1
        best_cand = None
        for cand in columns[i]:
            val = x_vars[(i, cand)].X
            if val > best_val:
                best_val = val
                best_cand = cand
        solution[i] = tuple(sorted(best_cand)) if best_cand else ()

    print(f"Done. Column generation took {iteration} iteration(s).")
    return solution


# --------------------
# Example usage (pseudo code; adapt paths and modules as needed)

if __name__ == "__main__":
    import data_io
    import heuristics
    import heuristics_variable_data_experiments as var

    # Parse scores from your Gobnilp .jkl file
    parsed_scores = data_io.parse_gobnilp_jkl('/home/gulce/Downloads/thesis/data/sachs/sachs_scores.jkl')
    scores = heuristics.GobnilpScores(parsed_scores)

    # Solve for some K > 3, e.g. K=4
    K = 4
    solution = maximize_true_graph_posterior(scores, scores.n, K)
    print("Solution:", solution)

    # Evaluate posterior of the found network
    bn = BayesianNetwork(solution, scores)
    posterior = bn.compute_posterior()
    print("Log Posterior of Found Network:", posterior)

    # Compare to the 'true' or reference network
    true_parents = var.get_true_parents('/home/gulce/Downloads/thesis/data/sachs/sachs.bif')
    bn_true = BayesianNetwork(true_parents, scores)
    print("True Parents:", true_parents)
    posterior_true = bn_true.compute_posterior()
    print("Log Posterior of Reference Network:", posterior_true)

    bn = BayesianNetwork({0: (2, 3), 1: (0, 2), 2: (0, 1), 3: (0, 2), 4: (2, 5), 5: (2, 3), 6: (2, 3), 7: (2, 5), 8: (9, 10), 9: (8, 10), 10: (8, 9)}, scores)
  
    posterior = bn.compute_posterior()
    print("Log of the Posterior Probability of the opT Network:", posterior)

import subprocess
def sample_from_exact_modular_sampler(jkl_file,n,output_file):

    # Define the command as a list of arguments
    command = ["/home/gulce/Downloads/thesis/modular-dag-sampling-master/sampler", "nonsymmetric", jkl_file, n]

    # Specify the output file
    

    # Run the command and write its output to the file
    with open(output_file, "w") as file:
        with open('/home/gulce/Downloads/thesis/data/child/error.log', 'w') as err_file:
            try:
                result = subprocess.run(command, check=True, text=True, stdout=file, stderr=err_file)
                print("Command executed successfully! Output written to", output_file)
            except subprocess.CalledProcessError as e:
                print("An error occurred while executing the command.")
                print("Error message:", e.stderr)
                
                
def sample_from_exact_modular_fair_sampler(n,m,output_file):

    # Define the command as a list of arguments
    command = ["/home/gulce/Downloads/thesis/modular-dag-sampling-master/sampler", "symmetric",'fair',  str(n),str(m)]

    # Specify the output file
    

    # Run the command and write its output to the file
    with open(output_file, "w") as file:
        with open('/home/gulce/Downloads/thesis/data/child/error.log', 'w') as err_file:
            try:
                result = subprocess.run(command, check=True, text=True, stdout=file, stderr=err_file)
                print("Command executed successfully! Output written to", output_file)
            except subprocess.CalledProcessError as e:
                print("An error occurred while executing the command.")
                print("Error message:", e.stderr)


            
            
# sample_from_exact_modular_sampler('/home/gulce/Downloads/thesis/data/synth/synt.jkl','10000','/home/gulce/Downloads/thesis/data/synt/synt_exact_sampled.txt')

import heuristics
import data_io
def mcmc_sample_pymc(jkl_file,n,output_file):
    scores=data_io.parse_gobnilp_jkl(jkl_file)
    parsed_scores=heuristics.GobnilpScores(scores)
    import logging
    import numpy as np
    import pymc as pm

# Setup logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

        
    import pymc as pm
    import numpy as np

    def build_model(scores_obj, num_nodes):
        with pm.Model() as model:
            parent_choices = {}

            # Use `scores_obj.local_scores` instead of iterating over `scores_obj`
            for node in range(num_nodes):
                if node in scores_obj.local_scores:
                    node_scores = scores_obj.local_scores[node]  # Extract dictionary for node
                    
                    jittered_probs = []
                    
                    for parents, score in node_scores.items():  # Iterate over (parent_set, score)
                        jittered_score = score #+ np.random.normal(0, 0.01)  # Adding jitter
                        jittered_probs.append(jittered_score)

                    # Normalize jittered scores to sum to 1
                    jittered_probs = np.clip(jittered_probs, 0, 1)  # Ensure within [0,1]
                    total_prob = np.sum(jittered_probs)
                    
                    if total_prob == 0:
                        jittered_probs = np.ones_like(jittered_probs) / len(jittered_probs)  # Use uniform
                    else:
                        jittered_probs /= total_prob  # Normalize

                    # Define categorical distribution
                    parent_choices[node] = pm.Categorical(f'parents_of_{node}', p=jittered_probs, shape=(1,))

        return model


    # Building the model with the possibility of random starts
    model = build_model(parsed_scores, num_nodes=parsed_scores.n)


    with model:
        # Using different step methods and increasing tune and sample sizes
        step = pm.Metropolis()  # Using Metropolis for potentially better exploration
        trace = pm.sample(n, tune=1000, step=step, return_inferencedata=False)




        # Extract sampled parent sets for analysis
        

    # Extract sampled parent sets for analysis
    sampled_parent_sets = {varname: trace.get_values(varname) for varname in model.named_vars}

    def decode_samples(sampled_parent_sets, scores):
        decoded_samples = set()  # Using a set to avoid duplicates

        for sample in range(len(sampled_parent_sets[list(sampled_parent_sets.keys())[0]])):
            dag = {}

            for node, choices in sampled_parent_sets.items():
                node_index = int(node.split('_')[-1])
                parent_index = choices[sample][0]  # Extract index from trace

                # Fix: Access `local_scores` properly
                try:
                    parents_dict = scores.local_scores[node_index]  # Get all parent sets for this node
                    parent_sets = list(parents_dict.keys())  # Get all parent sets as list
                    parents = parent_sets[parent_index]  # Select parent set by index
                except (KeyError, IndexError):
                    print(f"Warning: Invalid access for node {node_index} at index {parent_index}")
                    parents = ()

                dag[node_index] = frozenset(parents)

            decoded_samples.add(frozenset((k, frozenset(v)) for k, v in dag.items()))

        return decoded_samples


    decoded_dags = decode_samples(sampled_parent_sets, parsed_scores)



    def write_dags_to_file(dags, filename):
        with open(filename, 'w') as file:
            for dag_frozenset in dags:
                dag = {k: set(v) for k, v in dag_frozenset}
                dag_str = ', '.join(f"{node} <- {{{', '.join(map(str, sorted(parents)))}}}" 
                                    for node, parents in sorted(dag.items()))
                file.write(dag_str + "\n")

    write_dags_to_file(decoded_dags,output_file)
    def write_dags_to_file(dags, filename):
        with open(filename, 'w') as file:
            for dag_frozenset in dags:
                dag = {k: set(v) for k, v in dag_frozenset}
                dag_str = ', '.join(f"{node} <- {{{', '.join(map(str, sorted(parents)))}}}" 
                                    for node, parents in sorted(dag.items()))
                file.write(dag_str + "\n")

    write_dags_to_file(decoded_dags,output_file)
# mcmc_sample_pymc('/home/gulce/Downloads/thesis/data/sachs/sachs_scores.jkl',10000,'/home/gulce/Downloads/thesis/data/sachs/sachs_pymc_sampled_dags.txt')
# mcmc_sample_pymc('/home/gulce/Downloads/thesis/data/insurance/insurance_scores.jkl',10000,'/home/gulce/Downloads/thesis/data/insurance/insurance_pymc_sampled_dags.txt')


# mcmc_sample_pymc('/home/gulce/Downloads/thesis/data/child/child_scores.jkl',10000,'/home/gulce/Downloads/thesis/data/child/child_pymc_sampled_dags.txt')
#mcmc_sample_pymc('/home/gulce/Downloads/thesis/data/hailfinder/hailfinder_scores.jkl',10000,'/home/gulce/Downloads/thesis/data/hailfinder/hailfinder_pymc_sampled_dags_clean.txt')












def sample_from_naive_mcmc_sampler(jkl_file,burn_in,txt_file,n):
    # Define the command and arguments as a list
    cmd = [
        "Rscript",
        "/home/gulce/Downloads/thesis/mcmc_sampler.r",
        jkl_file,
        burn_in,
        txt_file,
        n
    ]

    # Run the command
    result = subprocess.run(cmd, capture_output=True, text=True)

    # Print the standard output and error (if any)
    print("Standard Output:")
    print(result.stdout)
    print("Standard Error:")
    print(result.stderr)
import data_io
import heuristics

parsed_scores = data_io.parse_gobnilp_jkl('/home/gulce/Downloads/thesis/data/sachs/sachs_scores.jkl')
scores = heuristics.GobnilpScores(parsed_scores)

import sumu
from sumu.candidates import candidate_parent_algorithm as cpa
cpa.keys()
import sys
import os
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# --------------------------------------------------------------------------
# 1. Optional Filtering Helper: top_n_algorithms
# --------------------------------------------------------------------------
def top_n_algorithms(df, sort_metric="F1", k_val=9, n=5):
    if sort_metric not in df.columns:
        print(f"[WARNING] sort_metric='{sort_metric}' not in columns. Skipping top_n filtering.")
        return df

    df_k = df[df['K'] == k_val].copy()
    if df_k.empty:
        print(f"[WARNING] No rows found where K={k_val}. Skipping top_n filtering.")
        return df

    df_k.sort_values(by=sort_metric, ascending=False, inplace=True)
    top_algos = df_k['Algorithm'].head(n).unique()
    print(f"[INFO] top_{n} algos by '{sort_metric}' at K={k_val}:", top_algos)

    return df[df['Algorithm'].isin(top_algos)].copy()


# --------------------------------------------------------------------------
# 2. Plot Approach A: Line Plot
# --------------------------------------------------------------------------
def plot_line(df, metric="Average Parent Coverage", xcol="K", save=None):
    df_sorted = df.copy()
    try:
        df_sorted[xcol] = df_sorted[xcol].astype(float)
    except ValueError:
        pass

    df_sorted.sort_values(by=xcol, inplace=True)

    plt.figure(figsize=(10, 6))
    sns.lineplot(data=df_sorted, x=xcol, y=metric, hue="Algorithm", marker="o")
    plt.title(f"Line Plot: {metric} vs. {xcol}")
    plt.tight_layout()
    
    if save:
        plt.savefig(save.replace('.csv','_line.png'))
        print(f"Plot saved as {save}")
    else:
        plt.show()


# --------------------------------------------------------------------------
# 3. Plot Approach B: Grouped Bar Chart
# --------------------------------------------------------------------------
def plot_bar(df, metric="Average Parent Coverage", xcol="K", save=None):
    df_bar = df.copy()
    try:
        df_bar[xcol] = df_bar[xcol].astype(float)
    except ValueError:
        pass

    df_bar.sort_values(by=xcol, inplace=True)

    plt.figure(figsize=(10, 6))
    ax = sns.barplot(data=df_bar, x=xcol, y=metric, hue="Algorithm", ci=None, edgecolor="black", width=0.8)
    plt.title(f"Grouped Bar: {metric} by {xcol} and Algorithm")
    plt.xticks(rotation=45)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)
    plt.tight_layout()

    if save:
        plt.savefig(save.replace('.csv','_bar.png'))
        print(f"Plot saved as {save}")
    else:
        plt.show()


# --------------------------------------------------------------------------
# 4. Plot Approach C: Facet Subplots
# --------------------------------------------------------------------------
def plot_facet(df, metric="Average Parent Coverage", xcol="K", save=None):
    df_facet = df.copy()
    try:
        df_facet[xcol] = df_facet[xcol].astype(float)
    except ValueError:
        pass

    g = sns.FacetGrid(df_facet, col="Algorithm", col_wrap=4, sharey=False, height=3)
    g.map_dataframe(sns.lineplot, x=xcol, y=metric, marker="o")
    g.set_titles(col_template="{col_name}")
    g.fig.suptitle(f"{metric} vs. {xcol} by Algorithm", y=1.05)
    plt.tight_layout()

    if save:
        plt.savefig(save.replace('.csv','_facet.png'))
        print(f"Plot saved as {save}")
    else:
        plt.show()


# --------------------------------------------------------------------------
# 5. Plot Approach D: Heatmap
# --------------------------------------------------------------------------
def plot_heatmap(df, metric="Average Parent Coverage", xcol="K",
                 heatmap_figsize=(30,24), save=None):
    """
    Creates a pivot table (Algorithm vs. K) of <metric> and displays
    as a heatmap, with a default figure size of (30,24).
    """
    df_hm = df.copy()
    try:
        df_hm[xcol] = df_hm[xcol].astype(float)
    except ValueError:
        pass

    pivoted = df_hm.pivot_table(index="Algorithm", columns=xcol, values=metric, aggfunc="mean")
    # Sort columns by ascending K
    pivoted = pivoted.reindex(sorted(pivoted.columns), axis=1)

    plt.figure(figsize=heatmap_figsize)
    sns.heatmap(pivoted, annot=True, fmt=".4f", cmap="YlGnBu",   annot_kws={"fontsize": 14} )
    plt.title(f"Heatmap of {metric} by Algorithm vs. {xcol}")
    plt.ylabel("Algorithm")
    plt.xlabel(xcol)
    plt.tight_layout()

    if save:
        outpath = save.replace('.csv','_heatmap.png')
        plt.savefig(outpath)
        print(f"[INFO] Heatmap plot saved as {outpath}")
    else:
        plt.show()


# --------------------------------------------------------------------------
# 6. Main Script
# --------------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description="Advanced coverage visualization with multiple plot types.")
    parser.add_argument("csv_file", help="Path to the coverage log CSV.")
    parser.add_argument("--plot_type", choices=["line", "bar", "facet", "heatmap"], default="bar", help="Choose one of: line, bar, facet, heatmap.")
    parser.add_argument("--metric", default="Average Parent Coverage", help="Which column to plot on the y-axis.")
    parser.add_argument("--k_col", default="K", help="Which column is used for the x-axis (default: K).")
    parser.add_argument("--sort_metric", default=None, help="If given, we pick top_n algorithms by this metric at K=k_max.")
    parser.add_argument("--k_max", type=float, default=5, help="Which K to use when picking top_n. Must be numeric. If not set, skip top_n.")
    parser.add_argument("--top_n", type=int, default=0, help="If >0, filter to top_n algorithms by --sort_metric at --k_max.")
    parser.add_argument("--exclude_consensus", action="store_true", help="If set, remove rows where Algorithm contains 'Consensus Parents'.")
    parser.add_argument("--save", type=str, help="Path to save the plot (e.g., 'plot.png').")

    args = parser.parse_args()

    # Load CSV
    if not os.path.isfile(args.csv_file):
        print(f"[ERROR] CSV file not found: {args.csv_file}")
        sys.exit(1)

    df = pd.read_csv(args.csv_file)
    print(f"[INFO] Loaded {df.shape[0]} rows from {args.csv_file}")
    print(f"[INFO] Columns: {df.columns.tolist()}")

    # Optionally exclude lines with "Consensus Parents"
    if args.exclude_consensus:
        before_count = df.shape[0]
        df = df[~df["Algorithm"].str.contains("Consensus Parents", na=False)].copy()
        after_count = df.shape[0]
        print(f"[INFO] Excluded consensus lines. Rows from {before_count} to {after_count}.")

    # top_n filtering if requested
    df = top_n_algorithms(df, sort_metric=args.sort_metric, k_val=args.k_max, n=5)

   

    plot_line(df, metric=args.metric, xcol=args.k_col, save=args.csv_file)

    plot_bar(df, metric=args.metric, xcol=args.k_col, save=args.csv_file)

    plot_facet(df, metric=args.metric, xcol=args.k_col, save=args.csv_file)

    plot_heatmap(df, metric=args.metric, xcol=args.k_col, save=args.csv_file)


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
import argparse
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def ensure_directory_exists(filepath):
    """Creates the directory for the filepath if it doesn't exist."""
    directory = os.path.dirname(filepath)
    if directory and not os.path.exists(directory):
        os.makedirs(directory)

def main():
    parser = argparse.ArgumentParser(
        description="Visualize CSV data as a heatmap of CoverageFraction by Algorithm and K."
    )
    parser.add_argument("csv_file", help="Path to the CSV file.")
    parser.add_argument("--metric", default="CoverageFraction",
                        help="CSV column to use as the heatmap values (default: CoverageFraction).")
    parser.add_argument("--xcol", default="K",
                        help="CSV column for the x-axis (default: K).")
    parser.add_argument("--ycol", default="Algorithm",
                        help="CSV column for the y-axis (default: Algorithm).")
    parser.add_argument("--save", type=str,
                        help="Optional path (including filename) to save the heatmap image (e.g., heatmap.png).")
    args = parser.parse_args()

    # Load the CSV file
    df = pd.read_csv(args.csv_file)
    
    # Pivot the DataFrame to have 'Algorithm' as rows and 'K' as columns with values from 'CoverageFraction'
    pivot_df = df.pivot_table(index=args.ycol, columns=args.xcol, values=args.metric, aggfunc="mean")
    
    # Sort the columns (K values) if they are numeric
    pivot_df = pivot_df.sort_index(axis=1)

    plt.figure(figsize=(8, 6))
    sns.heatmap(pivot_df, annot=True, fmt=".4f", cmap="YlGnBu")
    plt.title(f"Heatmap of {args.metric} by {args.ycol} vs. {args.xcol}")
    plt.xlabel(args.xcol)
    plt.ylabel(args.ycol)
    plt.tight_layout()
    
    if args.save:
        ensure_directory_exists(args.save)
        plt.savefig(args.save)
        print(f"Heatmap saved as {args.save}")
    else:
        plt.show()

if __name__ == "__main__":
    main()
